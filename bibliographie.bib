
@article{peteiro-barral_survey_2013,
	title = {A survey of methods for distributed machine learning},
	volume = {2},
	issn = {2192-6360},
	url = {https://doi.org/10.1007/s13748-012-0035-5},
	doi = {10.1007/s13748-012-0035-5},
	abstract = {Traditionally, a bottleneck preventing the development of more intelligent systems was the limited amount of data available. Nowadays, the total amount of information is almost incalculable and automatic data analyzers are even more needed. However, the limiting factor is the inability of learning algorithms to use all the data to learn within a reasonable time. In order to handle this problem, a new field in machine learning has emerged: large-scale learning. In this context, distributed learning seems to be a promising line of research since allocating the learning process among several workstations is a natural way of scaling up learning algorithms. Moreover, it allows to deal with data sets that are naturally distributed, a frequent situation in many real applications. This study provides some background regarding the advantages of distributed environments as well as an overview of distributed learning for dealing with “very large” data sets.},
	language = {en},
	number = {1},
	urldate = {2025-08-16},
	journal = {Progress in Artificial Intelligence},
	author = {Peteiro-Barral, Diego and Guijarro-Berdiñas, Bertha},
	month = mar,
	year = {2013},
	keywords = {Machine learning, Scalability, Data fragmentation, Distributed learning, Large-scale learning},
	pages = {1--11},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\CB33W3LW\\Peteiro-Barral and Guijarro-Berdiñas - 2013 - A survey of methods for distributed machine learning.pdf:application/pdf},
}

@article{kraska_mlbase_2013,
	title = {{MLbase}: {A} {Distributed} {Machine}-learning {System}},
	abstract = {Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelming—many users do not understand the trade-oﬀs and challenges of parameterizing and choosing between diﬀerent learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.},
	language = {en},
	author = {Kraska, Tim and Talwalkar, Ameet and Duchi, John},
	year = {2013},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\WVVIJJT8\\Kraska et al. - MLbase A Distributed Machine-learning System.pdf:application/pdf},
}

@incollection{galakatos_distributed_2017,
	title = {Distributed {Machine} {Learning}},
	isbn = {978-1-4899-7993-3},
	url = {https://link.springer.com/rwe/10.1007/978-1-4899-7993-3_80647-1},
	abstract = {'Distributed Machine Learning' published in 'Encyclopedia of Database Systems'},
	language = {en},
	urldate = {2025-08-16},
	booktitle = {Encyclopedia of {Database} {Systems}},
	publisher = {Springer, New York, NY},
	author = {Galakatos, Alex and Crotty, Andrew and Kraska, Tim},
	year = {2017},
	doi = {10.1007/978-1-4899-7993-3_80647-1},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\E5NNV83Z\\Galakatos et al. - 2017 - Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{sze_hardware_2017,
	title = {Hardware for machine learning: {Challenges} and opportunities},
	shorttitle = {Hardware for machine learning},
	url = {https://ieeexplore.ieee.org/abstract/document/7993626},
	doi = {10.1109/CICC.2017.7993626},
	abstract = {Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).},
	urldate = {2025-08-16},
	booktitle = {2017 {IEEE} {Custom} {Integrated} {Circuits} {Conference} ({CICC})},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Emer, Joel and Suleiman, Amr and Zhang, Zhengdong},
	month = apr,
	year = {2017},
	note = {ISSN: 2152-3630},
	keywords = {Hardware, Training, Cloud computing, Neural networks, Computer vision, Feature extraction, Robot sensing systems},
	pages = {1--8},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\BIGUL7R4\\7993626.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\5W2GQ5YD\\Sze et al. - 2017 - Hardware for machine learning Challenges and opportunities.pdf:application/pdf},
}

@article{lee_speeding_2018,
	title = {Speeding {Up} {Distributed} {Machine} {Learning} {Using} {Codes}},
	volume = {64},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/8002642},
	doi = {10.1109/TIT.2017.2736066},
	abstract = {Codes are widely used in many engineering applications to offer robustness against noise. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how coded solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: matrix multiplication and data shuffling. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is n , and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of łog n . For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction {\textbackslash}alpha of the data matrix can be cached at each worker, and n is the number of workers, coded shuffling reduces the communication cost by a factor of łeft({\textbackslash}alpha + {\textbackslash}frac 1n{\textbackslash}right){\textbackslash}gamma (n) compared with uncoded shuffling, where {\textbackslash}gamma (n) is the ratio of the cost of unicasting n messages to n users to multicasting a common message (of the same size) to n users. For instance, {\textbackslash}gamma (n) {\textbackslash}simeq n if multicasting a message to n users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.},
	number = {3},
	urldate = {2025-08-16},
	journal = {IEEE Transactions on Information Theory},
	author = {Lee, Kangwook and Lam, Maximilian and Pedarsani, Ramtin and Papailiopoulos, Dimitris and Ramchandran, Kannan},
	month = mar,
	year = {2018},
	keywords = {Runtime, Algorithm design and analysis, Robustness, Machine learning algorithms, channel coding, distributed computing, distributed databases, Distributed databases, encoding, Encoding, machine learning algorithms, multicast communication, Multicast communication, robustness, runtime},
	pages = {1514--1529},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\8RPFBD32\\8002642.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\LBSFDACP\\Lee et al. - 2018 - Speeding Up Distributed Machine Learning Using Codes.pdf:application/pdf},
}

@inproceedings{won_tacos_2024,
	title = {{TACOS}: {Topology}-{Aware} {Collective} {Algorithm} {Synthesizer} for {Distributed} {Machine} {Learning}},
	shorttitle = {{TACOS}},
	url = {https://ieeexplore.ieee.org/abstract/document/10764470},
	doi = {10.1109/MICRO61859.2024.00068},
	abstract = {The surge of artificial intelligence, particularly large language models, has driven the rapid development of large-scale machine learning clusters. Executing distributed models on these clusters is often constrained by communication overhead, making efficient utilization of available network resources crucial. As a result, the routing algorithm employed for collective communications (i.e., collective algorithms) plays a pivotal role in determining overall performance. Unfortunately, existing collective communication libraries for distributed machine learning are limited by a fixed set of basic collective algorithms. This limitation hinders communication optimization, especially in modern clusters with heterogeneous and asymmetric topologies. Furthermore, manually designing collective algorithms for all possible combinations of network topologies and collective patterns requires heavy engineering and validation efforts. To address these challenges, this paper presents Tacos, an autonomous synthesizer capable of automatically generating topology-aware collective algorithms tailored to specific collective patterns and network topologies. Tacos is highly flexible, synthesizing an All-Reduce algorithm for a heterogeneous 128-NPU system in just 1.08 seconds, while achieving up to a 4.27× performance improvement over state-of-the-art synthesizers. Additionally, Tacos demonstrates better scalability with polynomial synthesis times, in contrast to NP-hard approaches which only scale to systems with tens of NPUs. Tacos can synthesize for 40K NPUs in just 2.52 hours.},
	urldate = {2025-08-16},
	booktitle = {2024 57th {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Won, William and Elavazhagan, Midhilesh and Srinivasan, Sudarshan and Gupta, Swati and Krishna, Tushar},
	month = nov,
	year = {2024},
	note = {ISSN: 2379-3155},
	keywords = {Machine learning, Machine learning algorithms, Scalability, Clustering algorithms, collective algorithm synthesizer, collective communication, distributed machine learning, Network topology, Polynomials, Routing, Surges, Synthesizers, Topology},
	pages = {856--870},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\7P5NKJX5\\10764470.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\NTDBEQZQ\\Won et al. - 2024 - TACOS Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{zhao_adapcc_2024,
	title = {{AdapCC}: {Making} {Collective} {Communication} in {Distributed} {Machine} {Learning} {Adaptive}},
	shorttitle = {{AdapCC}},
	url = {https://ieeexplore.ieee.org/abstract/document/10631011},
	doi = {10.1109/ICDCS60910.2024.00012},
	abstract = {As deep learning (DL) models continue to grow in size, there is a pressing need for distributed model learning using a large number of devices (e.g., G PU s) and servers. Collective communication among devices/servers (for gradient synchronization, intermediate data exchange, etc.) introduces significant overheads, rendering major performance bottlenecks in distributed learning. A number of communication libraries, such as NCCL, Gloo and MPI, have been developed to optimize collective communication. Predefined communication strategies (e.g., ring or tree) are largely adopted, which may not be efficient or adaptive enough for inter-machine communication, especially in cloud-based scenarios where instance configurations and network performance can vary substantially. We propose AdapCC, a novel communication library that dynamically adapts to resource heterogeneity and network variability for optimized communication and training performance. AdapCC generates communication strategies based on run-time profiling, mitigates resource waste in waiting for computation stragglers, and executes efficient data transfers among DL workers. Experimental results under various settings demonstrate 2x communication speed-up and 31 \% training throughput improvement with AdapCC, as compared to NCCL and other representative communication backends.},
	urldate = {2025-08-16},
	booktitle = {2024 {IEEE} 44th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Zhao, Xiaoyang and Zhang, Zhe and Wu, Chuan},
	month = jul,
	year = {2024},
	note = {ISSN: 2575-8411},
	keywords = {Graphics processing units, Performance evaluation, Libraries, Throughput, Training, Rendering (computer graphics), collective communication, distributed training, Pressing},
	pages = {25--35},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\YU2HZZ4V\\Zhao et al. - 2024 - AdapCC Making Collective Communication in Distributed Machine Learning Adaptive.pdf:application/pdf},
}

@inproceedings{lee_model_2014,
	title = {On {Model} {Parallelization} and {Scheduling} {Strategies} for {Distributed} {Machine} {Learning}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/186b3d044a8c9898679d98dbd0d9b860-Abstract.html},
	abstract = {Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.},
	urldate = {2025-08-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Seunghak and Kim, Jin Kyu and Zheng, Xun and Ho, Qirong and Gibson, Garth A. and Xing, Eric P.},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\GEAIEKTR\\Lee et al. - 2014 - On Model Parallelization and Scheduling Strategies for Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{liu_distributed_2017,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '17 {Companion}},
	title = {Distributed {Machine} {Learning}: {Foundations}, {Trends}, and {Practices}},
	isbn = {978-1-4503-4914-7},
	shorttitle = {Distributed {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3041021.3051099},
	doi = {10.1145/3041021.3051099},
	abstract = {In recent years, artificial intelligence has achieved great success in many important applications. Both novel machine learning algorithms (e.g., deep neural networks), and their distributed implementations play very critical roles in the success. In this tutorial, we will first review popular machine learning algorithms and the optimization techniques they use. Second, we will introduce widely used ways of parallelizing machine learning algorithms (including both data parallelism and model parallelism, both synchronous and asynchronous parallelization), and discuss their theoretical properties, strengths, and weakness. Third, we will present some recent works that try to improve standard parallelization mechanisms. Last, we will provide some practical examples of parallelizing given machine learning algorithms in online application (e.g. Recommendation and Ranking) by using popular distributed platforms, such as Spark MlLib, DMTK, and Tensorflow. By listening to this tutorial, the audience can form a clear knowledge framework about distributed machine learning, and gain some hands-on experiences on parallelizing a given machine learning algorithm using popular distributed systems.},
	urldate = {2025-08-15},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web} {Companion}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Liu, Tie-Yan and Chen, Wei and Wang, Taifeng},
	month = apr,
	year = {2017},
	pages = {913--915},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\J9C759FB\\Liu et al. - 2017 - Distributed Machine Learning Foundations, Trends, and Practices.pdf:application/pdf},
}

@article{verbraeken_survey_2020,
	title = {A {Survey} on {Distributed} {Machine} {Learning}},
	volume = {53},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3377454},
	doi = {10.1145/3377454},
	abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
	number = {2},
	urldate = {2025-08-16},
	journal = {ACM Comput. Surv.},
	author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
	month = mar,
	year = {2020},
	pages = {30:1--30:33},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\Q7R8I5FF\\Verbraeken et al. - 2020 - A Survey on Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{jangda_breaking_2022,
	address = {New York, NY, USA},
	series = {{ASPLOS} '22},
	title = {Breaking the computation and communication abstraction barrier in distributed machine learning workloads},
	isbn = {978-1-4503-9205-1},
	url = {https://dl.acm.org/doi/10.1145/3503222.3507778},
	doi = {10.1145/3503222.3507778},
	abstract = {Recent trends towards large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, the current logical separation between computation and communication kernels in machine learning frameworks misses optimization opportunities across this barrier. Breaking this abstraction can provide many optimizations to improve the performance of distributed workloads. However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone.  Therefore, we present CoCoNet, which contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNet enabled us to optimize data-, model- and pipeline-parallel workloads in large language models with only a few lines of code. Our experiments show that CoCoNet significantly outperforms state-of-the-art distributed machine learning implementations.},
	urldate = {2025-08-15},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
	month = feb,
	year = {2022},
	pages = {402--416},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\MLJYJVZL\\Jangda et al. - 2022 - Breaking the computation and communication abstraction barrier in distributed machine learning workl.pdf:application/pdf},
}

@article{boehm_hybrid_2014,
	title = {Hybrid parallelization strategies for large-scale machine learning in {SystemML}},
	volume = {7},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2732286.2732292},
	doi = {10.14778/2732286.2732292},
	abstract = {SystemML aims at declarative, large-scale machine learning (ML) on top of MapReduce, where high-level ML scripts with R-like syntax are compiled to programs of MR jobs. The declarative specification of ML algorithms enables---in contrast to existing large-scale machine learning libraries---automatic optimization. SystemML's primary focus is on data parallelism but many ML algorithms inherently exhibit opportunities for task parallelism as well. A major challenge is how to efficiently combine both types of parallelism for arbitrary ML scripts and workloads. In this paper, we present a systematic approach for combining task and data parallelism for large-scale machine learning on top of MapReduce. We employ a generic Parallel FOR construct (ParFOR) as known from high performance computing (HPC). Our core contributions are (1) complementary parallelization strategies for exploiting multi-core and cluster parallelism, as well as (2) a novel cost-based optimization framework for automatically creating optimal parallel execution plans. Experiments on a variety of use cases showed that this achieves both efficiency and scalability due to automatic adaptation to ad-hoc workloads and unknown data characteristics.},
	number = {7},
	urldate = {2025-08-16},
	journal = {Proc. VLDB Endow.},
	author = {Boehm, Matthias and Tatikonda, Shirish and Reinwald, Berthold and Sen, Prithviraj and Tian, Yuanyuan and Burdick, Douglas R. and Vaithyanathan, Shivakumar},
	month = mar,
	year = {2014},
	pages = {553--564},
	file = {Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\89H9CE8G\\Boehm et al. - 2014 - Hybrid parallelization strategies for large-scale machine learning in SystemML.pdf:application/pdf},
}

@misc{tang_communication-efficient_2023,
	title = {Communication-{Efficient} {Distributed} {Deep} {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Communication-{Efficient} {Distributed} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2003.06307},
	doi = {10.48550/arXiv.2003.06307},
	abstract = {Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we conduct extensive experiments to empirically compare the convergence performance of various mainstream distributed training algorithms. Based on our system-level communication cost analysis, theoretical and experimental convergence speed comparison, we provide readers with an understanding of which algorithms are more efficient under specific distributed environments. Our research also extrapolates potential directions for further optimizations.},
	urldate = {2025-08-16},
	publisher = {arXiv},
	author = {Tang, Zhenheng and Shi, Shaohuai and Wang, Wei and Li, Bo and Chu, Xiaowen},
	month = sep,
	year = {2023},
	note = {arXiv:2003.06307 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\R7MR2SMK\\Tang et al. - 2023 - Communication-Efficient Distributed Deep Learning A Comprehensive Survey.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\QMRQAJGD\\2003.html:text/html},
}

@inproceedings{li_communication_2014,
	title = {Communication {Efficient} {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/935ad074f32d1e8f085a143449894cdc-Abstract.html},
	urldate = {2025-08-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Mu and Andersen, David G. and Smola, Alexander and Yu, Kai},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\ICGTCHXH\\Li et al. - 2014 - Communication Efficient Distributed Machine Learning with the Parameter Server.pdf:application/pdf},
}

@article{abramson_pattern_1963,
	title = {Pattern recognition and machine learning},
	volume = {9},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057854/},
	doi = {10.1109/TIT.1963.1057854},
	language = {en},
	number = {4},
	urldate = {2025-08-16},
	journal = {IEEE Transactions on Information Theory},
	author = {Abramson, N. and Braverman, D. and Sebestyen, G.},
	month = oct,
	year = {1963},
	pages = {257--261},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\IXHNCDWZ\\Abramson et al. - 1963 - Pattern recognition and machine learning.pdf:application/pdf},
}

@article{samuel_studies_1959,
	title = {Some {Studies} in {Machine} {Learning} {Using} the {Game} of {Checkers}},
	volume = {3},
	issn = {0018-8646},
	url = {https://ieeexplore.ieee.org/document/5392560},
	doi = {10.1147/rd.33.0210},
	abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
	number = {3},
	urldate = {2025-08-16},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A. L.},
	month = jul,
	year = {1959},
	pages = {210--229},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\Q5VWMGV6\\Samuel - 1959 - Some Studies in Machine Learning Using the Game of Checkers.pdf:application/pdf},
}

@article{priyadarshini_impact_2024,
	title = {The {Impact} of {User} {Interface} {Design} on {User} {Engagement}},
	volume = {13},
	abstract = {In the digital era, user engagement stands as a critical metric for the success of digital products and platforms, with user interface (UI) design playing a pivotal role in shaping engagement levels. This abstract delves into the multifaceted relationship between UI design and user engagement, synthesizing existing literature and empirical findings to elucidate key insights and implications. Research indicates that UI design significantly influences user engagement through various factors such as usability, accessibility, visual aesthetics, interactivity, and personalization. Intuitive layouts, clear navigation, appealing visual aesthetics, and responsive interactions contribute to seamless user experiences that captivate users' attention and encourage interaction. Furthermore, personalization features, social integration, and emotional design elements enhance engagement by fostering a sense of connection, belonging, and satisfaction. However, challenges remain in understanding the nuanced interplay between design factors and user behaviors, particularly in diverse cultural contexts and emerging technologies. This abstract calls for continued research efforts to deepen our understanding of UI design principles and their implications for user engagement, urging interdisciplinary collaborations and innovative methodologies to address current gaps and challenges.},
	language = {en},
	number = {03},
	journal = {International Journal of Engineering Research},
	author = {Priyadarshini, Ar Poorva},
	year = {2024},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\9SKCHQQC\\Priyadarshini - 2024 - The Impact of User Interface Design on User Engagement.pdf:application/pdf},
}

@inproceedings{nielsen_introduction_1998,
	address = {New York, NY, USA},
	series = {{CHI} '98},
	title = {Introduction to {Web} design},
	isbn = {978-1-58113-028-7},
	url = {https://dl.acm.org/doi/10.1145/286498.286557},
	doi = {10.1145/286498.286557},
	urldate = {2025-08-16},
	booktitle = {{CHI} 98 {Conference} {Summary} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nielsen, Jakob},
	month = apr,
	year = {1998},
	pages = {107--108},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\68Y5CUU4\\Nielsen - 1998 - Introduction to Web design.pdf:application/pdf},
}

@inproceedings{nielsen_heuristic_1990,
	address = {New York, NY, USA},
	series = {{CHI} '90},
	title = {Heuristic evaluation of user interfaces},
	isbn = {978-0-201-50932-8},
	url = {https://dl.acm.org/doi/10.1145/97243.97281},
	doi = {10.1145/97243.97281},
	abstract = {Heuristic evaluation is an informal method of usability analysis where a number of evaluators are presented with an interface design and asked to comment on it. Four experiments showed that individual evaluators were mostly quite bad at doing such heuristic evaluations and that they only found between 20 and 51\% of the usability problems in the interfaces they evaluated. On the other hand, we could aggregate the evaluations from several evaluators to a single evaluation and such aggregates do rather well, even when they consist of only three to five people.},
	urldate = {2025-08-16},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nielsen, Jakob and Molich, Rolf},
	month = mar,
	year = {1990},
	pages = {249--256},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\IMLDEAY4\\Nielsen and Molich - 1990 - Heuristic evaluation of user interfaces.pdf:application/pdf},
}

@article{diehl_defining_2022,
	title = {Defining {Recommendations} to {Guide} {User} {Interface} {Design}: {Multimethod} {Approach}},
	volume = {9},
	issn = {2292-9495},
	shorttitle = {Defining {Recommendations} to {Guide} {User} {Interface} {Design}},
	url = {https://humanfactors.jmir.org/2022/3/e37894},
	doi = {10.2196/37894},
	abstract = {Background
              For the development of digital solutions, different aspects of user interface design must be taken into consideration. Different technologies, interaction paradigms, user characteristics and needs, and interface design components are some of the aspects that designers and developers should pay attention to when designing a solution. Many user interface design recommendations for different digital solutions and user profiles are found in the literature, but these recommendations have numerous similarities, contradictions, and different levels of detail. A detailed critical analysis is needed that compares, evaluates, and validates existing recommendations and allows the definition of a practical set of recommendations.
            
            
              Objective
              This study aimed to analyze and synthesize existing user interface design recommendations and propose a practical set of recommendations that guide the development of different technologies.
            
            
              Methods
              Based on previous studies, a set of recommendations on user interface design was generated following 4 steps: (1) interview with user interface design experts; (2) analysis of the experts’ feedback and drafting of a set of recommendations; (3) reanalysis of the shorter list of recommendations by a group of experts; and (4) refining and finalizing the list.
            
            
              Results
              The findings allowed us to define a set of 174 recommendations divided into 12 categories, according to usability principles, and organized into 2 levels of hierarchy: generic (69 recommendations) and specific (105 recommendations).
            
            
              Conclusions
              This study shows that user interface design recommendations can be divided according to usability principles and organized into levels of detail. Moreover, this study reveals that some recommendations, as they address different technologies and interaction paradigms, need further work.},
	language = {en},
	number = {3},
	urldate = {2025-08-16},
	journal = {JMIR Human Factors},
	author = {Diehl, Ceci and Martins, Ana and Almeida, Ana and Silva, Telmo and Ribeiro, Óscar and Santinha, Gonçalo and Rocha, Nelson and Silva, Anabela G},
	month = sep,
	year = {2022},
	pages = {e37894},
	annote = {[TLDR] This study shows that user interface design recommendations can be divided according to usability principles and organized into levels of detail, and reveals that some recommendations, as they address different technologies and interaction paradigms, need further work.},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\UE4T924T\\Diehl et al. - 2022 - Defining Recommendations to Guide User Interface Design Multimethod Approach.pdf:application/pdf},
}

@inproceedings{grechenig_human_1993,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human {Computer} {Interaction}: {Vienna} {Conference}, {VCHCI} '93, {Fin} de {Siècle} {Vienna}, {Austria}, {September} 20–22, 1993 {Proceedings}},
	volume = {733},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-540-57312-8 978-3-540-48052-5},
	shorttitle = {Human {Computer} {Interaction}},
	url = {http://link.springer.com/10.1007/3-540-57312-7},
	doi = {10.1007/3-540-57312-7},
	abstract = {The intention of this paper is to provide an overview on the subject of Human-Computer Interaction(HCI). Human-computer interaction basically covers the concepts of humans interacting with computers, but computers do not understand our feelings or emotions, so we need to inform them of how they should react in different situations, and to help the computer understand different situations, we use various techniques. In these different techniques, principles are designed for the interaction of a human and a computer in such a manner that our expectations are met. Additionally, we can define HCI as the area of study where only the approaches, principles, and techniques are applied to build a user-friendly interface between people and computers. Because we are all 
surrounded by many devices that make our jobs easier, HCI is crucial in our daily lives. Therefore, HCI is the end result of ongoing testing and improvement of interface designs that may have an impact on the context of usage for users.},
	language = {en},
	urldate = {2025-08-16},
	publisher = {Springer Berlin Heidelberg},
	editor = {Grechenig, Thomas and Tscheligi, Manfred and Goos, Gerhard and Hartmanis, Juris},
	year = {1993},
	doi = {10.1007/3-540-57312-7},
	annote = {[TLDR] Human-Computer Interaction is the area of study where only the approaches, principles, and techniques are applied to build a user-friendly interface between people and computers.},
}

@book{grechenig_human_1993-1,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human {Computer} {Interaction}: {Vienna} {Conference}, {VCHCI} '93, {Fin} de {Siècle} {Vienna}, {Austria}, {September} 20–22, 1993 {Proceedings}},
	volume = {733},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-540-57312-8 978-3-540-48052-5},
	shorttitle = {Human {Computer} {Interaction}},
	url = {http://link.springer.com/10.1007/3-540-57312-7},
	language = {en},
	urldate = {2025-08-16},
	publisher = {Springer},
	editor = {Grechenig, Thomas and Tscheligi, Manfred and Goos, Gerhard and Hartmanis, Juris},
	year = {1993},
	doi = {10.1007/3-540-57312-7},
	keywords = {Benutzungsschnittstelle, communication, Computer Supported Cooperation, Graphical User Interface, Human Computer Interaction, human-computer interaction (HCI), Hypermedia, Mensch-Computer Schnittstelle, Multimedia, Multimediea, proving, real-time, User Interface, User Interface Design, visualization},
}

@book{shneiderman_designing_2017,
	address = {Boston, Mass. [u.a]},
	edition = {Sixth edition},
	series = {Always {Learning}},
	title = {Designing the {User} {Interface}: strategies for effective human-computer interaction},
	isbn = {978-0-13-438038-4},
	shorttitle = {Designing the {User} {Interface}},
	language = {en},
	publisher = {Pearson Education (US)},
	author = {Shneiderman, Ben and Plaisant, Catherine and Cohen, Maxine and Jacobs, Steven and Elmqvist, Niklas and Diakopoulos, Nicholas},
	year = {2017},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\877NAW5I\\Shneiderman and Plaisant - 2004 - Designing the user interface strategies for effective human-computer interaction.pdf:application/pdf},
}

@article{alpaydin_machine_2021,
	title = {Machine {Learning}},
	language = {en},
	journal = {MACHINE LEARNING},
	author = {Alpaydin, Ethem},
	year = {2021},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\TNMY9JPG\\Alpaydin - Machine Learning.pdf:application/pdf},
}

@incollection{turing_computing_1989,
	address = {USA},
	title = {Computing machinery and intelligence (1950)},
	isbn = {978-0-89391-369-4},
	urldate = {2025-08-16},
	booktitle = {Perspectives on the computer revolution},
	publisher = {Ablex Publishing Corp.},
	author = {Turing, A. M.},
	month = jun,
	year = {1989},
	pages = {85--107},
	file = {turing:C\:\\Users\\debrouwe\\Zotero\\storage\\SHQMA2LV\\turing.pdf:application/pdf},
}

@inproceedings{reddi_mlperf_2020,
	address = {Valencia, Spain},
	title = {{MLPerf} {Inference} {Benchmark}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-4661-4},
	url = {https://ieeexplore.ieee.org/document/9138989/},
	doi = {10.1109/ISCA45697.2020.00045},
	abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and ﬁve orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing MLsystem performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The ﬁrst call for submissions garnered more than 600 reproducible inferenceperformance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark’s ﬂexibility and adaptability.},
	language = {en},
	urldate = {2025-08-17},
	booktitle = {2020 {ACM}/{IEEE} 47th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
	month = may,
	year = {2020},
	pages = {446--459},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\Y5HRLMTB\\Reddi et al. - 2020 - MLPerf Inference Benchmark.pdf:application/pdf},
}

@inproceedings{hirofuchi_adding_2013,
	title = {Adding {Virtual} {Machine} {Abstractions} {Into} {SimGrid}: {A} {First} {Step} {Toward} the {Simulation} of {Infrastructure}-as-a-{Service} {Concerns}},
	shorttitle = {Adding {Virtual} {Machine} {Abstractions} {Into} {SimGrid}},
	url = {https://ieeexplore.ieee.org/document/6686025},
	doi = {10.1109/CGC.2013.33},
	abstract = {As real systems become larger and more complex, the use of simulator frameworks grows in our research community. By leveraging them, users can focus on the major aspects of their algorithm, run in-siclo experiments (i.e., simulations), and thoroughly analyze results, even for a large-scale environment without facing the complexity of conducting in-vivo studies (i.e., on real test beds). Since nowadays the virtual machine (VM) technology has become a fundamental building block of distributed computing environments, in particular in cloud infrastructures, our community needs a full-fledged simulation framework that enables us to investigate large-scale virtualized environments through accurate simulations. To be adopted, such a framework should provides easy-to-use APIs, close to the real ones and preferably fully compatible with those of an existing popular simulation framework. In this paper, we present the current implementation status of a highly-scalable and versatile simulation framework supporting VM environments, extending a widely-used, open-source framework, SimGrid. Our simulation framework allows users to launch hundreds of thousands of VMs on their simulation programs and control VMs in the same manner as in the real world (e.g., suspend/resume and migrate). Users can execute computation and communication tasks on physical machines (PMs) and VMs through the same SimGrid API, which will provide a seamless migration path to IaaS simulations for thousands of SimGrid users. Preliminary validations showed that the resource sharing mechanism of the VM support worked correctly.},
	urldate = {2025-08-17},
	booktitle = {2013 {International} {Conference} on {Cloud} and {Green} {Computing}},
	author = {Hirofuchi, Takahiro and Lebre, Adrien},
	month = sep,
	year = {2013},
	keywords = {Workstations, Computational modeling, Processor scheduling, Virtual machining, Cloud computing, Engines, CLoud Computing, Communities, Simulation, Virtual Machine},
	pages = {175--180},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\2R6QK96H\\Hirofuchi and Lebre - 2013 - Adding Virtual Machine Abstractions Into SimGrid A First Step Toward the Simulation of Infrastructu.pdf:application/pdf},
}

@article{tran_user-centered_2024,
	title = {User-centered {Web} {Design} and {Accessibility}},
	url = {https://www.theseus.fi/bitstream/handle/10024/869788/Tran_Y.pdf?sequence=2},
	language = {en},
	author = {Tran, Y},
	year = {2024},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\QUQPZKH9\\Tran - User-centered Web Design and Accessibility.pdf:application/pdf},
}

@book{bekkerman_scaling_2011,
	title = {Scaling up {Machine} {Learning}: {Parallel} and {Distributed} {Approaches}},
	isbn = {978-1-139-50190-3},
	url = {https://books.google.com/books?id=9u0gAwAAQBAJ},
	publisher = {Cambridge University Press},
	author = {Bekkerman, R. and Bilenko, M. and Langford, J.},
	year = {2011},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\ALLK476X\\Bekkerman et al. - Scaling Up Machine Learning Parallel and Distributed Approaches.pdf:application/pdf},
}

@incollection{rojas_backpropagation_1996,
	address = {Berlin, Heidelberg},
	title = {The {Backpropagation} {Algorithm}},
	isbn = {978-3-642-61068-4},
	url = {https://doi.org/10.1007/978-3-642-61068-4_7},
	abstract = {We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems—the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.},
	language = {en},
	urldate = {2025-08-17},
	booktitle = {Neural {Networks}: {A} {Systematic} {Introduction}},
	publisher = {Springer},
	author = {Rojas, Raúl},
	editor = {Rojas, Raúl},
	year = {1996},
	doi = {10.1007/978-3-642-61068-4_7},
	pages = {149--182},
}

@article{perez_karich_emergence_2025,
	title = {Emergence and {Evolution} of ‘{Big} {Data}’ {Research}: {A} 30-{Year} {Scientometric} {Analysis} of the {Knowledge} {Field}},
	volume = {2},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {3042-5042},
	shorttitle = {Emergence and {Evolution} of ‘{Big} {Data}’ {Research}},
	url = {https://www.mdpi.com/3042-5042/2/3/15},
	doi = {10.3390/metrics2030015},
	abstract = {In the ongoing ‘data revolution’, the ubiquity of digital data in society underlines a transformative era. This is mirrored in the sciences, where ‘big data’ has emerged as a major research field. This article significantly extends previous scientometric analyses by tracing the field’s conceptual emergence and evolution across a 30-year period (1993–2022). Bibliometric analysis is based on 17 data categories that co-constitute the conceptual network of ‘big data’ research. Using Scopus, the search query resulted in 70,163 articles and 315,235 author keywords. These are analysed aggregately regarding co-occurrences of the 17 data categories and co-occurrences of data categories with author keywords, and regarding their disciplinary distributions and interdisciplinary reach. Temporal analysis reveals two major development phases: 1993–2012 and 2013–2022. The study demonstrates: (1) the rapid expansion of the research field concentrated on seven main data categories; (2) the consolidation of keyword (co-)occurrences on ‘machine learning’, ‘deep learning’, ‘artificial intelligence’ and ‘cloud computing’; and (3) significant interdisciplinarity across four main subject areas. Scholars can use the findings to combine data categories and author keywords in ways that align scholarly work with specific thematic and disciplinary interests. The findings could also inform research funding, especially concerning opportunities for cross-disciplinary research.},
	language = {en},
	number = {3},
	urldate = {2025-08-17},
	journal = {Metrics},
	author = {Perez Karich, Ignacio and Joss, Simon},
	month = sep,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cloud computing, bibliometrics, big data, deep learning, digital data, intelligent data, machine learning, novel data, scientometrics, social media data},
	pages = {15},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\GER4LNH9\\Perez Karich and Joss - 2025 - Emergence and Evolution of ‘Big Data’ Research A 30-Year Scientometric Analysis of the Knowledge Fi.pdf:application/pdf},
}

@incollection{oneto_limitations_2020,
	address = {Cham},
	title = {Limitations of {Shallow} {Networks}},
	volume = {896},
	isbn = {978-3-030-43882-1 978-3-030-43883-8},
	url = {http://link.springer.com/10.1007/978-3-030-43883-8_6},
	abstract = {Although originally biologically inspired neural networks were introduced as multilayer computational models, shallow networks have been dominant in applications till the recent renewal of interest in deep architectures. Experimental evidence and successful applications of deep networks pose theoretical questions asking: When and why are deep networks better than shallow ones? This chapter presents some probabilistic and constructive results on limitations of shallow networks. It shows implications of geometrical properties of high-dimensional spaces for probabilistic lower bounds on network complexity. The bounds depend on covering numbers of dictionaries of computational units and sizes of domains of functions to be computed. Probabilistic results are complemented by constructive ones built using Hadamard matrices and pseudo-noise sequences.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {Recent {Trends} in {Learning} {From} {Data}},
	publisher = {Springer International Publishing},
	author = {Kůrková, Věra},
	editor = {Oneto, Luca and Navarin, Nicolò and Sperduti, Alessandro and Anguita, Davide},
	year = {2020},
	doi = {10.1007/978-3-030-43883-8_6},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {129--154},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\LRDPPLMW\\Kůrková - 2020 - Limitations of Shallow Networks.pdf:application/pdf},
}

@article{theis_end_2017,
	title = {The {End} of {Moore}'s {Law}: {A} {New} {Beginning} for {Information} {Technology}},
	volume = {19},
	issn = {1558-366X},
	shorttitle = {The {End} of {Moore}'s {Law}},
	url = {https://ieeexplore.ieee.org/abstract/document/7878935},
	doi = {10.1109/MCSE.2017.29},
	abstract = {The insights contained in Gordon Moore's now famous 1965 and 1975 papers have broadly guided the development of semiconductor electronics for over 50 years. However, the field-effect transistor is approaching some physical limits to further miniaturization, and the associated rising costs and reduced return on investment appear to be slowing the pace of development. Far from signaling an end to progress, this gradual "end of Moore's law" will open a new era in information technology as the focus of research and development shifts from miniaturization of long-established technologies to the coordinated introduction of new devices, new integration technologies, and new architectures for computing.},
	number = {2},
	urldate = {2025-08-18},
	journal = {Computing in Science \& Engineering},
	author = {Theis, Thomas N. and Wong, H.-S. Philip},
	month = mar,
	year = {2017},
	keywords = {Computer architecture, Memory management, Random access memory, Scientific computing, Algorithm design and analysis, algorithms implemented in hardware, emerging technologies, Field effect transistors, introductory and survey, memory technologies, Moore's Law, neural nets, scientific computing, Switching circuits},
	pages = {41--50},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\Z2DFSC9Y\\7878935.html:text/html},
}

@inproceedings{li_understanding_2024,
	address = {Sydney Australia},
	title = {Understanding {Communication} {Characteristics} of {Distributed} {Training}},
	isbn = {979-8-4007-1758-1},
	url = {https://dl.acm.org/doi/10.1145/3663408.3663409},
	doi = {10.1145/3663408.3663409},
	abstract = {Communication is pivotal in distributed training and a thorough understanding of its characteristics is essential for future optimizations. However, prior works are limited, either focusing on customized optimizations or conducting incomplete explorations on communication characteristics. In this work, we systematically analyze the communication characteristics of distributed training, considering two key aspects of communication: pattern and overhead, and assessing a broad spectrum of determinant factors. In particular, we extensively investigate the features of communication patterns, such as predictability, and comprehensively evaluate the impact of various factors on communication overhead. Additionally, we develop and validate an analytical formulation to estimate communication overhead, providing a mathematical understanding of models with predictability.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {Proceedings of the 8th {Asia}-{Pacific} {Workshop} on {Networking}},
	publisher = {ACM},
	author = {Li, Wenxue and Liu, Xiangzhou and Li, Yuxuan and Jin, Yilun and Tian, Han and Zhong, Zhizhen and Liu, Guyue and Zhang, Ying and Chen, Kai},
	month = aug,
	year = {2024},
	pages = {1--8},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\HV8B6SL3\\Li et al. - 2024 - Understanding Communication Characteristics of Distributed Training.pdf:application/pdf},
}

@article{moore_cramming_2006,
	title = {Cramming more components onto integrated circuits, {Reprinted} from {Electronics}, volume 38, number 8, {April} 19, 1965, pp.114 ff},
	volume = {11},
	doi = {10.1109/N-SSC.2006.4785860},
	journal = {Solid-State Circuits Newsletter, IEEE},
	author = {Moore, Gordon},
	month = oct,
	year = {2006},
	pages = {33 -- 35},
	file = {102770836-05-01-acc:C\:\\Users\\debrouwe\\Zotero\\storage\\3XG3AWCE\\102770836-05-01-acc.pdf:application/pdf},
}

@inproceedings{sevilla_compute_2022,
	title = {Compute {Trends} {Across} {Three} {Eras} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/2202.05924},
	doi = {10.1109/IJCNN55064.2022.9891914},
	abstract = {Compute, data, and algorithmic advances are the three fundamental factors that guide the progress of modern Machine Learning (ML). In this paper we study trends in the most readily quantiﬁed factor – compute. We show that before 2010 training compute grew in line with Moore’s law, doubling roughly every 20 months. Since the advent of Deep Learning in the early 2010s, the scaling of training compute has accelerated, doubling approximately every 6 months. In late 2015, a new trend emerged as ﬁrms developed large-scale ML models with 10 to 100-fold larger requirements in training compute. Based on these observations we split the history of compute in ML into three eras: the Pre Deep Learning Era , the Deep Learning Era and the Large-Scale Era . Overall, our work highlights the fast-growing compute requirements for training advanced ML systems.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
	month = jul,
	year = {2022},
	note = {arXiv:2202.05924 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	pages = {1--8},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\XLQX6F7X\\Sevilla et al. - 2022 - Compute Trends Across Three Eras of Machine Learning.pdf:application/pdf},
}

@inproceedings{jiang_megascale_2024,
	address = {Santa Clara, CA},
	title = {{MegaScale}: {Scaling} {Large} {Language} {Model} {Training} to {More} {Than} 10,000 {GPUs}},
	isbn = {978-1-939133-39-7},
	url = {https://www.usenix.org/conference/nsdi24/presentation/jiang-ziheng},
	booktitle = {21st {USENIX} {Symposium} on {Networked} {Systems} {Design} and {Implementation} ({NSDI} 24)},
	publisher = {USENIX Association},
	author = {Jiang, Ziheng and Lin, Haibin and Zhong, Yinmin and Huang, Qi and Chen, Yangrui and Zhang, Zhi and Peng, Yanghua and Li, Xiang and Xie, Cong and Nong, Shibiao and Jia, Yulu and He, Sun and Chen, Hongmin and Bai, Zhihao and Hou, Qi and Yan, Shipeng and Zhou, Ding and Sheng, Yiyao and Jiang, Zhuo and Xu, Haohan and Wei, Haoran and Zhang, Zhang and Nie, Pengfei and Zou, Leqi and Zhao, Sida and Xiang, Liang and Liu, Zherui and Li, Zhe and Jia, Xiaoying and Ye, Jianxi and Jin, Xin and Liu, Xin},
	month = apr,
	year = {2024},
	pages = {745--760},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\MKX3ZGWB\\Jiang et al. - MegaScale Scaling Large Language Model Training to More Than 10,000 GPUs.pdf:application/pdf},
}

@inproceedings{zhe_fan_gpu_2004,
	address = {Pittsburgh, PA, USA},
	title = {{GPU} {Cluster} for {High} {Performance} {Computing}},
	isbn = {978-0-7695-2153-4},
	url = {http://ieeexplore.ieee.org/document/1392977/},
	doi = {10.1109/SC.2004.26},
	abstract = {Inspired by the attractive Flops/dollar ratio and the incredible growth in the speed of modern graphics processing units (GPUs), we propose to use a cluster of GPUs for high performance scientiﬁc computing. As an example application, we have developed a parallel ﬂow simulation using the lattice Boltzmann model (LBM) on a GPU cluster and have simulated the dispersion of airborne contaminants in the Times Square area of New York City. Using 30 GPU nodes, our simulation can compute a 480x400x80 LBM in 0.31 second/step, a speed which is 4.6 times faster than that of our CPU cluster implementation. Besides the LBM, we also discuss other potential applications of the GPU cluster, such as cellular automata, PDE solvers, and FEM.},
	language = {en},
	urldate = {2025-08-21},
	booktitle = {Proceedings of the {ACM}/{IEEE} {SC2004} {Conference}},
	publisher = {IEEE},
	author = {{Zhe Fan} and {Feng Qiu} and Kaufman, A. and Yoakum-Stover, S.},
	year = {2004},
	pages = {47--47},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\8AQ8PBAH\\Zhe Fan et al. - 2004 - GPU Cluster for High Performance Computing.pdf:application/pdf},
}

@article{xing_strategies_2016,
	title = {Strategies and {Principles} of {Distributed} {Machine} {Learning} on {Big} {Data}},
	volume = {2},
	issn = {2095-8099},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809916309468},
	doi = {10.1016/J.ENG.2016.02.008},
	abstract = {The rise of big data has led to new demands for machine learning (ML) systems to learn complex models, with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics (such as high-dimensional latent features, intermediate representations, and decision functions) thereupon. In order to run ML algorithms at such scales, on a distributed cluster with tens to thousands of machines, it is often the case that significant engineering efforts are required—and one might fairly ask whether such engineering truly falls within the domain of ML research. Taking the view that “big” ML systems can benefit greatly from ML-rooted statistical and algorithmic insights—and that ML researchers should therefore not shy away from such systems design—we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of big ML systems and architectures, with the goal of understanding how to make them efficient, generally applicable, and supported with convergence and scaling guarantees. They concern four key questions that traditionally receive little attention in ML research: How can an ML program be distributed over a cluster? How can ML computation be bridged with inter-machine communication? How can such communication be performed? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems.},
	number = {2},
	urldate = {2025-08-21},
	journal = {Engineering},
	author = {Xing, Eric P. and Ho, Qirong and Xie, Pengtao and Wei, Dai},
	month = jun,
	year = {2016},
	keywords = {Distributed systems, Machine learning, Artificial intelligence big data, Big model, Data-parallelism, Model-parallelism, Principles, Theory},
	pages = {179--195},
	file = {ScienceDirect Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\F2DQSRD4\\S2095809916309468.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\XMRJTBJP\\Xing et al. - 2016 - Strategies and Principles of Distributed Machine Learning on Big Data.pdf:application/pdf},
}

@misc{dong_towards_2024,
	title = {Towards {Low}-bit {Communication} for {Tensor} {Parallel} {LLM} {Inference}},
	url = {http://arxiv.org/abs/2411.07942},
	doi = {10.48550/arXiv.2411.07942},
	abstract = {Tensor parallelism provides an effective way to increase server large language model (LLM) inference efficiency despite adding an additional communication cost. However, as server LLMs continue to scale in size, they will need to be distributed across more devices, magnifying the communication cost. One way to approach this problem is with quantization, but current methods for LLMs tend to avoid quantizing the features that tensor parallelism needs to communicate. Taking advantage of consistent outliers in communicated features, we introduce a quantization method that reduces communicated values on average from 16 bits to 4.2 bits while preserving nearly all of the original performance. For instance, our method maintains around 98.0\% and 99.5\% of Gemma 2 27B's and Llama 2 13B's original performance, respectively, averaged across all tasks we evaluated on.},
	urldate = {2025-08-21},
	publisher = {arXiv},
	author = {Dong, Harry and Johnson, Tyler and Cho, Minsik and Soroush, Emad},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07942 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\UBXZXVZ7\\Dong et al. - 2024 - Towards Low-bit Communication for Tensor Parallel LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\X58U5CVT\\2411.html:text/html},
}

@inproceedings{lian_asynchronous_2018,
	title = {Asynchronous {Decentralized} {Parallel} {Stochastic} {Gradient} {Descent}},
	url = {https://proceedings.mlr.press/v80/lian18a.html},
	abstract = {Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal O(1/K−−√)O(1/K)O(1/{\textbackslash}sqrt\{K\}) rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8x faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.},
	language = {en},
	urldate = {2025-08-21},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3043--3052},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\W6CCHJBH\\Lian et al. - 2018 - Asynchronous Decentralized Parallel Stochastic Gradient Descent.pdf:application/pdf;Supplementary PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\C4YSLYPS\\Lian et al. - 2018 - Asynchronous Decentralized Parallel Stochastic Gradient Descent.pdf:application/pdf},
}

@misc{ratnasamy_collective_2025,
	address = {Berkeley, CA},
	type = {Lecture},
	title = {Collective {Operations}},
	url = {https://textbook.cs168.io/beyond-client-server/collective-operations.html},
	language = {en},
	urldate = {2025-08-21},
	author = {Ratnasamy, : Sylvia and Shakir, Rob and Peyrin, Kao},
	year = {2025},
	file = {[CS168 SP25] Lecture 25 - Collectives (1) (1):C\:\\Users\\debrouwe\\Zotero\\storage\\KWE5PWEH\\[CS168 SP25] Lecture 25 - Collectives (1) (1).pdf:application/pdf},
}
