
@article{peteiro-barral_survey_2013,
	title = {A survey of methods for distributed machine learning},
	volume = {2},
	issn = {2192-6360},
	url = {https://doi.org/10.1007/s13748-012-0035-5},
	doi = {10.1007/s13748-012-0035-5},
	abstract = {Traditionally, a bottleneck preventing the development of more intelligent systems was the limited amount of data available. Nowadays, the total amount of information is almost incalculable and automatic data analyzers are even more needed. However, the limiting factor is the inability of learning algorithms to use all the data to learn within a reasonable time. In order to handle this problem, a new field in machine learning has emerged: large-scale learning. In this context, distributed learning seems to be a promising line of research since allocating the learning process among several workstations is a natural way of scaling up learning algorithms. Moreover, it allows to deal with data sets that are naturally distributed, a frequent situation in many real applications. This study provides some background regarding the advantages of distributed environments as well as an overview of distributed learning for dealing with “very large” data sets.},
	language = {en},
	number = {1},
	urldate = {2025-08-16},
	journal = {Progress in Artificial Intelligence},
	author = {Peteiro-Barral, Diego and Guijarro-Berdiñas, Bertha},
	month = mar,
	year = {2013},
	keywords = {Machine learning, Scalability, Data fragmentation, Distributed learning, Large-scale learning},
	pages = {1--11},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\CB33W3LW\\Peteiro-Barral and Guijarro-Berdiñas - 2013 - A survey of methods for distributed machine learning.pdf:application/pdf},
}

@article{kraska_mlbase_2013,
	title = {{MLbase}: {A} {Distributed} {Machine}-learning {System}},
	abstract = {Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelming—many users do not understand the trade-oﬀs and challenges of parameterizing and choosing between diﬀerent learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.},
	language = {en},
	author = {Kraska, Tim and Talwalkar, Ameet and Duchi, John},
	year = {2013},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\WVVIJJT8\\Kraska et al. - MLbase A Distributed Machine-learning System.pdf:application/pdf},
}

@incollection{galakatos_distributed_2017,
	title = {Distributed {Machine} {Learning}},
	isbn = {978-1-4899-7993-3},
	url = {https://link.springer.com/rwe/10.1007/978-1-4899-7993-3_80647-1},
	abstract = {'Distributed Machine Learning' published in 'Encyclopedia of Database Systems'},
	language = {en},
	urldate = {2025-08-16},
	booktitle = {Encyclopedia of {Database} {Systems}},
	publisher = {Springer, New York, NY},
	author = {Galakatos, Alex and Crotty, Andrew and Kraska, Tim},
	year = {2017},
	doi = {10.1007/978-1-4899-7993-3_80647-1},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\E5NNV83Z\\Galakatos et al. - 2017 - Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{sze_hardware_2017,
	title = {Hardware for machine learning: {Challenges} and opportunities},
	shorttitle = {Hardware for machine learning},
	url = {https://ieeexplore.ieee.org/abstract/document/7993626},
	doi = {10.1109/CICC.2017.7993626},
	abstract = {Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or limitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).},
	urldate = {2025-08-16},
	booktitle = {2017 {IEEE} {Custom} {Integrated} {Circuits} {Conference} ({CICC})},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Emer, Joel and Suleiman, Amr and Zhang, Zhengdong},
	month = apr,
	year = {2017},
	note = {ISSN: 2152-3630},
	keywords = {Hardware, Training, Cloud computing, Neural networks, Computer vision, Feature extraction, Robot sensing systems},
	pages = {1--8},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\BIGUL7R4\\7993626.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\5W2GQ5YD\\Sze et al. - 2017 - Hardware for machine learning Challenges and opportunities.pdf:application/pdf},
}

@article{lee_speeding_2018,
	title = {Speeding {Up} {Distributed} {Machine} {Learning} {Using} {Codes}},
	volume = {64},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/8002642},
	doi = {10.1109/TIT.2017.2736066},
	abstract = {Codes are widely used in many engineering applications to offer robustness against noise. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how coded solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: matrix multiplication and data shuffling. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is n , and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of łog n . For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction {\textbackslash}alpha of the data matrix can be cached at each worker, and n is the number of workers, coded shuffling reduces the communication cost by a factor of łeft({\textbackslash}alpha + {\textbackslash}frac 1n{\textbackslash}right){\textbackslash}gamma (n) compared with uncoded shuffling, where {\textbackslash}gamma (n) is the ratio of the cost of unicasting n messages to n users to multicasting a common message (of the same size) to n users. For instance, {\textbackslash}gamma (n) {\textbackslash}simeq n if multicasting a message to n users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.},
	number = {3},
	urldate = {2025-08-16},
	journal = {IEEE Transactions on Information Theory},
	author = {Lee, Kangwook and Lam, Maximilian and Pedarsani, Ramtin and Papailiopoulos, Dimitris and Ramchandran, Kannan},
	month = mar,
	year = {2018},
	keywords = {Runtime, Algorithm design and analysis, Robustness, Machine learning algorithms, channel coding, distributed computing, distributed databases, Distributed databases, encoding, Encoding, machine learning algorithms, multicast communication, Multicast communication, robustness, runtime},
	pages = {1514--1529},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\8RPFBD32\\8002642.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\LBSFDACP\\Lee et al. - 2018 - Speeding Up Distributed Machine Learning Using Codes.pdf:application/pdf},
}

@inproceedings{won_tacos_2024,
	title = {{TACOS}: {Topology}-{Aware} {Collective} {Algorithm} {Synthesizer} for {Distributed} {Machine} {Learning}},
	shorttitle = {{TACOS}},
	url = {https://ieeexplore.ieee.org/abstract/document/10764470},
	doi = {10.1109/MICRO61859.2024.00068},
	abstract = {The surge of artificial intelligence, particularly large language models, has driven the rapid development of large-scale machine learning clusters. Executing distributed models on these clusters is often constrained by communication overhead, making efficient utilization of available network resources crucial. As a result, the routing algorithm employed for collective communications (i.e., collective algorithms) plays a pivotal role in determining overall performance. Unfortunately, existing collective communication libraries for distributed machine learning are limited by a fixed set of basic collective algorithms. This limitation hinders communication optimization, especially in modern clusters with heterogeneous and asymmetric topologies. Furthermore, manually designing collective algorithms for all possible combinations of network topologies and collective patterns requires heavy engineering and validation efforts. To address these challenges, this paper presents Tacos, an autonomous synthesizer capable of automatically generating topology-aware collective algorithms tailored to specific collective patterns and network topologies. Tacos is highly flexible, synthesizing an All-Reduce algorithm for a heterogeneous 128-NPU system in just 1.08 seconds, while achieving up to a 4.27× performance improvement over state-of-the-art synthesizers. Additionally, Tacos demonstrates better scalability with polynomial synthesis times, in contrast to NP-hard approaches which only scale to systems with tens of NPUs. Tacos can synthesize for 40K NPUs in just 2.52 hours.},
	urldate = {2025-08-16},
	booktitle = {2024 57th {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Won, William and Elavazhagan, Midhilesh and Srinivasan, Sudarshan and Gupta, Swati and Krishna, Tushar},
	month = nov,
	year = {2024},
	note = {ISSN: 2379-3155},
	keywords = {Machine learning, Machine learning algorithms, Scalability, Clustering algorithms, collective algorithm synthesizer, collective communication, distributed machine learning, Network topology, Polynomials, Routing, Surges, Synthesizers, Topology},
	pages = {856--870},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\7P5NKJX5\\10764470.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\NTDBEQZQ\\Won et al. - 2024 - TACOS Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{zhao_adapcc_2024,
	title = {{AdapCC}: {Making} {Collective} {Communication} in {Distributed} {Machine} {Learning} {Adaptive}},
	shorttitle = {{AdapCC}},
	url = {https://ieeexplore.ieee.org/abstract/document/10631011},
	doi = {10.1109/ICDCS60910.2024.00012},
	abstract = {As deep learning (DL) models continue to grow in size, there is a pressing need for distributed model learning using a large number of devices (e.g., G PU s) and servers. Collective communication among devices/servers (for gradient synchronization, intermediate data exchange, etc.) introduces significant overheads, rendering major performance bottlenecks in distributed learning. A number of communication libraries, such as NCCL, Gloo and MPI, have been developed to optimize collective communication. Predefined communication strategies (e.g., ring or tree) are largely adopted, which may not be efficient or adaptive enough for inter-machine communication, especially in cloud-based scenarios where instance configurations and network performance can vary substantially. We propose AdapCC, a novel communication library that dynamically adapts to resource heterogeneity and network variability for optimized communication and training performance. AdapCC generates communication strategies based on run-time profiling, mitigates resource waste in waiting for computation stragglers, and executes efficient data transfers among DL workers. Experimental results under various settings demonstrate 2x communication speed-up and 31 \% training throughput improvement with AdapCC, as compared to NCCL and other representative communication backends.},
	urldate = {2025-08-16},
	booktitle = {2024 {IEEE} 44th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Zhao, Xiaoyang and Zhang, Zhe and Wu, Chuan},
	month = jul,
	year = {2024},
	note = {ISSN: 2575-8411},
	keywords = {Graphics processing units, Performance evaluation, Libraries, Throughput, Training, Rendering (computer graphics), collective communication, distributed training, Pressing},
	pages = {25--35},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\YU2HZZ4V\\Zhao et al. - 2024 - AdapCC Making Collective Communication in Distributed Machine Learning Adaptive.pdf:application/pdf},
}

@inproceedings{lee_model_2014,
	title = {On {Model} {Parallelization} and {Scheduling} {Strategies} for {Distributed} {Machine} {Learning}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/186b3d044a8c9898679d98dbd0d9b860-Abstract.html},
	abstract = {Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.},
	urldate = {2025-08-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Seunghak and Kim, Jin Kyu and Zheng, Xun and Ho, Qirong and Gibson, Garth A. and Xing, Eric P.},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\GEAIEKTR\\Lee et al. - 2014 - On Model Parallelization and Scheduling Strategies for Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{liu_distributed_2017,
	address = {Republic and Canton of Geneva, CHE},
	series = {{WWW} '17 {Companion}},
	title = {Distributed {Machine} {Learning}: {Foundations}, {Trends}, and {Practices}},
	isbn = {978-1-4503-4914-7},
	shorttitle = {Distributed {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3041021.3051099},
	doi = {10.1145/3041021.3051099},
	abstract = {In recent years, artificial intelligence has achieved great success in many important applications. Both novel machine learning algorithms (e.g., deep neural networks), and their distributed implementations play very critical roles in the success. In this tutorial, we will first review popular machine learning algorithms and the optimization techniques they use. Second, we will introduce widely used ways of parallelizing machine learning algorithms (including both data parallelism and model parallelism, both synchronous and asynchronous parallelization), and discuss their theoretical properties, strengths, and weakness. Third, we will present some recent works that try to improve standard parallelization mechanisms. Last, we will provide some practical examples of parallelizing given machine learning algorithms in online application (e.g. Recommendation and Ranking) by using popular distributed platforms, such as Spark MlLib, DMTK, and Tensorflow. By listening to this tutorial, the audience can form a clear knowledge framework about distributed machine learning, and gain some hands-on experiences on parallelizing a given machine learning algorithm using popular distributed systems.},
	urldate = {2025-08-15},
	booktitle = {Proceedings of the 26th {International} {Conference} on {World} {Wide} {Web} {Companion}},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Liu, Tie-Yan and Chen, Wei and Wang, Taifeng},
	month = apr,
	year = {2017},
	pages = {913--915},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\J9C759FB\\Liu et al. - 2017 - Distributed Machine Learning Foundations, Trends, and Practices.pdf:application/pdf},
}

@article{verbraeken_survey_2020,
	title = {A {Survey} on {Distributed} {Machine} {Learning}},
	volume = {53},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3377454},
	doi = {10.1145/3377454},
	abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
	number = {2},
	urldate = {2025-08-16},
	journal = {ACM Comput. Surv.},
	author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
	month = mar,
	year = {2020},
	pages = {30:1--30:33},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\Q7R8I5FF\\Verbraeken et al. - 2020 - A Survey on Distributed Machine Learning.pdf:application/pdf},
}

@inproceedings{jangda_breaking_2022,
	address = {New York, NY, USA},
	series = {{ASPLOS} '22},
	title = {Breaking the computation and communication abstraction barrier in distributed machine learning workloads},
	isbn = {978-1-4503-9205-1},
	url = {https://dl.acm.org/doi/10.1145/3503222.3507778},
	doi = {10.1145/3503222.3507778},
	abstract = {Recent trends towards large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, the current logical separation between computation and communication kernels in machine learning frameworks misses optimization opportunities across this barrier. Breaking this abstraction can provide many optimizations to improve the performance of distributed workloads. However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone.  Therefore, we present CoCoNet, which contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNet enabled us to optimize data-, model- and pipeline-parallel workloads in large language models with only a few lines of code. Our experiments show that CoCoNet significantly outperforms state-of-the-art distributed machine learning implementations.},
	urldate = {2025-08-15},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
	month = feb,
	year = {2022},
	pages = {402--416},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\MLJYJVZL\\Jangda et al. - 2022 - Breaking the computation and communication abstraction barrier in distributed machine learning workl.pdf:application/pdf},
}

@article{boehm_hybrid_2014,
	title = {Hybrid parallelization strategies for large-scale machine learning in {SystemML}},
	volume = {7},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/2732286.2732292},
	doi = {10.14778/2732286.2732292},
	abstract = {SystemML aims at declarative, large-scale machine learning (ML) on top of MapReduce, where high-level ML scripts with R-like syntax are compiled to programs of MR jobs. The declarative specification of ML algorithms enables---in contrast to existing large-scale machine learning libraries---automatic optimization. SystemML's primary focus is on data parallelism but many ML algorithms inherently exhibit opportunities for task parallelism as well. A major challenge is how to efficiently combine both types of parallelism for arbitrary ML scripts and workloads. In this paper, we present a systematic approach for combining task and data parallelism for large-scale machine learning on top of MapReduce. We employ a generic Parallel FOR construct (ParFOR) as known from high performance computing (HPC). Our core contributions are (1) complementary parallelization strategies for exploiting multi-core and cluster parallelism, as well as (2) a novel cost-based optimization framework for automatically creating optimal parallel execution plans. Experiments on a variety of use cases showed that this achieves both efficiency and scalability due to automatic adaptation to ad-hoc workloads and unknown data characteristics.},
	number = {7},
	urldate = {2025-08-16},
	journal = {Proc. VLDB Endow.},
	author = {Boehm, Matthias and Tatikonda, Shirish and Reinwald, Berthold and Sen, Prithviraj and Tian, Yuanyuan and Burdick, Douglas R. and Vaithyanathan, Shivakumar},
	month = mar,
	year = {2014},
	pages = {553--564},
	file = {Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\89H9CE8G\\Boehm et al. - 2014 - Hybrid parallelization strategies for large-scale machine learning in SystemML.pdf:application/pdf},
}

@misc{tang_communication-efficient_2023,
	title = {Communication-{Efficient} {Distributed} {Deep} {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Communication-{Efficient} {Distributed} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2003.06307},
	doi = {10.48550/arXiv.2003.06307},
	abstract = {Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we conduct extensive experiments to empirically compare the convergence performance of various mainstream distributed training algorithms. Based on our system-level communication cost analysis, theoretical and experimental convergence speed comparison, we provide readers with an understanding of which algorithms are more efficient under specific distributed environments. Our research also extrapolates potential directions for further optimizations.},
	urldate = {2025-08-16},
	publisher = {arXiv},
	author = {Tang, Zhenheng and Shi, Shaohuai and Wang, Wei and Li, Bo and Chu, Xiaowen},
	month = sep,
	year = {2023},
	note = {arXiv:2003.06307 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\R7MR2SMK\\Tang et al. - 2023 - Communication-Efficient Distributed Deep Learning A Comprehensive Survey.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\QMRQAJGD\\2003.html:text/html},
}

@inproceedings{li_communication_2014,
	title = {Communication {Efficient} {Distributed} {Machine} {Learning} with the {Parameter} {Server}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/935ad074f32d1e8f085a143449894cdc-Abstract.html},
	urldate = {2025-08-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Mu and Andersen, David G. and Smola, Alexander and Yu, Kai},
	year = {2014},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\ICGTCHXH\\Li et al. - 2014 - Communication Efficient Distributed Machine Learning with the Parameter Server.pdf:application/pdf},
}

@article{abramson_pattern_1963,
	title = {Pattern recognition and machine learning},
	volume = {9},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1057854/},
	doi = {10.1109/TIT.1963.1057854},
	language = {en},
	number = {4},
	urldate = {2025-08-16},
	journal = {IEEE Transactions on Information Theory},
	author = {Abramson, N. and Braverman, D. and Sebestyen, G.},
	month = oct,
	year = {1963},
	pages = {257--261},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\IXHNCDWZ\\Abramson et al. - 1963 - Pattern recognition and machine learning.pdf:application/pdf},
}

@article{samuel_studies_1959,
	title = {Some {Studies} in {Machine} {Learning} {Using} the {Game} of {Checkers}},
	volume = {3},
	issn = {0018-8646},
	url = {https://ieeexplore.ieee.org/document/5392560},
	doi = {10.1147/rd.33.0210},
	abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
	number = {3},
	urldate = {2025-08-16},
	journal = {IBM Journal of Research and Development},
	author = {Samuel, A. L.},
	month = jul,
	year = {1959},
	pages = {210--229},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\Q5VWMGV6\\Samuel - 1959 - Some Studies in Machine Learning Using the Game of Checkers.pdf:application/pdf},
}

@article{priyadarshini_impact_2024,
	title = {The {Impact} of {User} {Interface} {Design} on {User} {Engagement}},
	volume = {13},
	abstract = {In the digital era, user engagement stands as a critical metric for the success of digital products and platforms, with user interface (UI) design playing a pivotal role in shaping engagement levels. This abstract delves into the multifaceted relationship between UI design and user engagement, synthesizing existing literature and empirical findings to elucidate key insights and implications. Research indicates that UI design significantly influences user engagement through various factors such as usability, accessibility, visual aesthetics, interactivity, and personalization. Intuitive layouts, clear navigation, appealing visual aesthetics, and responsive interactions contribute to seamless user experiences that captivate users' attention and encourage interaction. Furthermore, personalization features, social integration, and emotional design elements enhance engagement by fostering a sense of connection, belonging, and satisfaction. However, challenges remain in understanding the nuanced interplay between design factors and user behaviors, particularly in diverse cultural contexts and emerging technologies. This abstract calls for continued research efforts to deepen our understanding of UI design principles and their implications for user engagement, urging interdisciplinary collaborations and innovative methodologies to address current gaps and challenges.},
	language = {en},
	number = {03},
	journal = {International Journal of Engineering Research},
	author = {Priyadarshini, Ar Poorva},
	year = {2024},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\9SKCHQQC\\Priyadarshini - 2024 - The Impact of User Interface Design on User Engagement.pdf:application/pdf},
}

@inproceedings{nielsen_introduction_1998,
	address = {New York, NY, USA},
	series = {{CHI} '98},
	title = {Introduction to {Web} design},
	isbn = {978-1-58113-028-7},
	url = {https://dl.acm.org/doi/10.1145/286498.286557},
	doi = {10.1145/286498.286557},
	urldate = {2025-08-16},
	booktitle = {{CHI} 98 {Conference} {Summary} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nielsen, Jakob},
	month = apr,
	year = {1998},
	pages = {107--108},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\68Y5CUU4\\Nielsen - 1998 - Introduction to Web design.pdf:application/pdf},
}

@inproceedings{nielsen_heuristic_1990,
	address = {New York, NY, USA},
	series = {{CHI} '90},
	title = {Heuristic evaluation of user interfaces},
	isbn = {978-0-201-50932-8},
	url = {https://dl.acm.org/doi/10.1145/97243.97281},
	doi = {10.1145/97243.97281},
	abstract = {Heuristic evaluation is an informal method of usability analysis where a number of evaluators are presented with an interface design and asked to comment on it. Four experiments showed that individual evaluators were mostly quite bad at doing such heuristic evaluations and that they only found between 20 and 51\% of the usability problems in the interfaces they evaluated. On the other hand, we could aggregate the evaluations from several evaluators to a single evaluation and such aggregates do rather well, even when they consist of only three to five people.},
	urldate = {2025-08-16},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nielsen, Jakob and Molich, Rolf},
	month = mar,
	year = {1990},
	pages = {249--256},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\IMLDEAY4\\Nielsen and Molich - 1990 - Heuristic evaluation of user interfaces.pdf:application/pdf},
}

@article{diehl_defining_2022,
	title = {Defining {Recommendations} to {Guide} {User} {Interface} {Design}: {Multimethod} {Approach}},
	volume = {9},
	issn = {2292-9495},
	shorttitle = {Defining {Recommendations} to {Guide} {User} {Interface} {Design}},
	url = {https://humanfactors.jmir.org/2022/3/e37894},
	doi = {10.2196/37894},
	abstract = {Background
              For the development of digital solutions, different aspects of user interface design must be taken into consideration. Different technologies, interaction paradigms, user characteristics and needs, and interface design components are some of the aspects that designers and developers should pay attention to when designing a solution. Many user interface design recommendations for different digital solutions and user profiles are found in the literature, but these recommendations have numerous similarities, contradictions, and different levels of detail. A detailed critical analysis is needed that compares, evaluates, and validates existing recommendations and allows the definition of a practical set of recommendations.
            
            
              Objective
              This study aimed to analyze and synthesize existing user interface design recommendations and propose a practical set of recommendations that guide the development of different technologies.
            
            
              Methods
              Based on previous studies, a set of recommendations on user interface design was generated following 4 steps: (1) interview with user interface design experts; (2) analysis of the experts’ feedback and drafting of a set of recommendations; (3) reanalysis of the shorter list of recommendations by a group of experts; and (4) refining and finalizing the list.
            
            
              Results
              The findings allowed us to define a set of 174 recommendations divided into 12 categories, according to usability principles, and organized into 2 levels of hierarchy: generic (69 recommendations) and specific (105 recommendations).
            
            
              Conclusions
              This study shows that user interface design recommendations can be divided according to usability principles and organized into levels of detail. Moreover, this study reveals that some recommendations, as they address different technologies and interaction paradigms, need further work.},
	language = {en},
	number = {3},
	urldate = {2025-08-16},
	journal = {JMIR Human Factors},
	author = {Diehl, Ceci and Martins, Ana and Almeida, Ana and Silva, Telmo and Ribeiro, Óscar and Santinha, Gonçalo and Rocha, Nelson and Silva, Anabela G},
	month = sep,
	year = {2022},
	pages = {e37894},
	annote = {[TLDR] This study shows that user interface design recommendations can be divided according to usability principles and organized into levels of detail, and reveals that some recommendations, as they address different technologies and interaction paradigms, need further work.},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\UE4T924T\\Diehl et al. - 2022 - Defining Recommendations to Guide User Interface Design Multimethod Approach.pdf:application/pdf},
}

@inproceedings{grechenig_human_1993,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human {Computer} {Interaction}: {Vienna} {Conference}, {VCHCI} '93, {Fin} de {Siècle} {Vienna}, {Austria}, {September} 20–22, 1993 {Proceedings}},
	volume = {733},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-540-57312-8 978-3-540-48052-5},
	shorttitle = {Human {Computer} {Interaction}},
	url = {http://link.springer.com/10.1007/3-540-57312-7},
	doi = {10.1007/3-540-57312-7},
	abstract = {The intention of this paper is to provide an overview on the subject of Human-Computer Interaction(HCI). Human-computer interaction basically covers the concepts of humans interacting with computers, but computers do not understand our feelings or emotions, so we need to inform them of how they should react in different situations, and to help the computer understand different situations, we use various techniques. In these different techniques, principles are designed for the interaction of a human and a computer in such a manner that our expectations are met. Additionally, we can define HCI as the area of study where only the approaches, principles, and techniques are applied to build a user-friendly interface between people and computers. Because we are all 
surrounded by many devices that make our jobs easier, HCI is crucial in our daily lives. Therefore, HCI is the end result of ongoing testing and improvement of interface designs that may have an impact on the context of usage for users.},
	language = {en},
	urldate = {2025-08-16},
	publisher = {Springer Berlin Heidelberg},
	editor = {Grechenig, Thomas and Tscheligi, Manfred and Goos, Gerhard and Hartmanis, Juris},
	year = {1993},
	doi = {10.1007/3-540-57312-7},
	annote = {[TLDR] Human-Computer Interaction is the area of study where only the approaches, principles, and techniques are applied to build a user-friendly interface between people and computers.},
}

@book{grechenig_human_1993-1,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human {Computer} {Interaction}: {Vienna} {Conference}, {VCHCI} '93, {Fin} de {Siècle} {Vienna}, {Austria}, {September} 20–22, 1993 {Proceedings}},
	volume = {733},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-540-57312-8 978-3-540-48052-5},
	shorttitle = {Human {Computer} {Interaction}},
	url = {http://link.springer.com/10.1007/3-540-57312-7},
	language = {en},
	urldate = {2025-08-16},
	publisher = {Springer},
	editor = {Grechenig, Thomas and Tscheligi, Manfred and Goos, Gerhard and Hartmanis, Juris},
	year = {1993},
	doi = {10.1007/3-540-57312-7},
	keywords = {Benutzungsschnittstelle, communication, Computer Supported Cooperation, Graphical User Interface, Human Computer Interaction, human-computer interaction (HCI), Hypermedia, Mensch-Computer Schnittstelle, Multimedia, Multimediea, proving, real-time, User Interface, User Interface Design, visualization},
}

@book{shneiderman_designing_2017,
	address = {Boston, Mass. [u.a]},
	edition = {Sixth edition},
	series = {Always {Learning}},
	title = {Designing the {User} {Interface}: strategies for effective human-computer interaction},
	isbn = {978-0-13-438038-4},
	shorttitle = {Designing the {User} {Interface}},
	language = {en},
	publisher = {Pearson Education (US)},
	author = {Shneiderman, Ben and Plaisant, Catherine and Cohen, Maxine and Jacobs, Steven and Elmqvist, Niklas and Diakopoulos, Nicholas},
	year = {2017},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\877NAW5I\\Shneiderman and Plaisant - 2004 - Designing the user interface strategies for effective human-computer interaction.pdf:application/pdf},
}

@article{alpaydin_machine_2021,
	title = {Machine {Learning}},
	language = {en},
	journal = {MACHINE LEARNING},
	author = {Alpaydin, Ethem},
	year = {2021},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\TNMY9JPG\\Alpaydin - Machine Learning.pdf:application/pdf},
}

@incollection{turing_computing_1989,
	address = {USA},
	title = {Computing machinery and intelligence (1950)},
	isbn = {978-0-89391-369-4},
	urldate = {2025-08-16},
	booktitle = {Perspectives on the computer revolution},
	publisher = {Ablex Publishing Corp.},
	author = {Turing, A. M.},
	month = jun,
	year = {1989},
	pages = {85--107},
	file = {turing:C\:\\Users\\debrouwe\\Zotero\\storage\\SHQMA2LV\\turing.pdf:application/pdf},
}

@inproceedings{reddi_mlperf_2020,
	address = {Valencia, Spain},
	title = {{MLPerf} {Inference} {Benchmark}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-7281-4661-4},
	url = {https://ieeexplore.ieee.org/document/9138989/},
	doi = {10.1109/ISCA45697.2020.00045},
	abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and ﬁve orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing MLsystem performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The ﬁrst call for submissions garnered more than 600 reproducible inferenceperformance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark’s ﬂexibility and adaptability.},
	language = {en},
	urldate = {2025-08-17},
	booktitle = {2020 {ACM}/{IEEE} 47th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
	month = may,
	year = {2020},
	pages = {446--459},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\Y5HRLMTB\\Reddi et al. - 2020 - MLPerf Inference Benchmark.pdf:application/pdf},
}

@inproceedings{hirofuchi_adding_2013,
	title = {Adding {Virtual} {Machine} {Abstractions} {Into} {SimGrid}: {A} {First} {Step} {Toward} the {Simulation} of {Infrastructure}-as-a-{Service} {Concerns}},
	shorttitle = {Adding {Virtual} {Machine} {Abstractions} {Into} {SimGrid}},
	url = {https://ieeexplore.ieee.org/document/6686025},
	doi = {10.1109/CGC.2013.33},
	abstract = {As real systems become larger and more complex, the use of simulator frameworks grows in our research community. By leveraging them, users can focus on the major aspects of their algorithm, run in-siclo experiments (i.e., simulations), and thoroughly analyze results, even for a large-scale environment without facing the complexity of conducting in-vivo studies (i.e., on real test beds). Since nowadays the virtual machine (VM) technology has become a fundamental building block of distributed computing environments, in particular in cloud infrastructures, our community needs a full-fledged simulation framework that enables us to investigate large-scale virtualized environments through accurate simulations. To be adopted, such a framework should provides easy-to-use APIs, close to the real ones and preferably fully compatible with those of an existing popular simulation framework. In this paper, we present the current implementation status of a highly-scalable and versatile simulation framework supporting VM environments, extending a widely-used, open-source framework, SimGrid. Our simulation framework allows users to launch hundreds of thousands of VMs on their simulation programs and control VMs in the same manner as in the real world (e.g., suspend/resume and migrate). Users can execute computation and communication tasks on physical machines (PMs) and VMs through the same SimGrid API, which will provide a seamless migration path to IaaS simulations for thousands of SimGrid users. Preliminary validations showed that the resource sharing mechanism of the VM support worked correctly.},
	urldate = {2025-08-17},
	booktitle = {2013 {International} {Conference} on {Cloud} and {Green} {Computing}},
	author = {Hirofuchi, Takahiro and Lebre, Adrien},
	month = sep,
	year = {2013},
	keywords = {Workstations, Computational modeling, Processor scheduling, Virtual machining, Cloud computing, Engines, CLoud Computing, Communities, Simulation, Virtual Machine},
	pages = {175--180},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\2R6QK96H\\Hirofuchi and Lebre - 2013 - Adding Virtual Machine Abstractions Into SimGrid A First Step Toward the Simulation of Infrastructu.pdf:application/pdf},
}

@article{tran_user-centered_2024,
	title = {User-centered {Web} {Design} and {Accessibility}},
	url = {https://www.theseus.fi/bitstream/handle/10024/869788/Tran_Y.pdf?sequence=2},
	language = {en},
	author = {Tran, Y},
	year = {2024},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\QUQPZKH9\\Tran - User-centered Web Design and Accessibility.pdf:application/pdf},
}

@book{bekkerman_scaling_2011,
	title = {Scaling up {Machine} {Learning}: {Parallel} and {Distributed} {Approaches}},
	isbn = {978-1-139-50190-3},
	url = {https://books.google.com/books?id=9u0gAwAAQBAJ},
	publisher = {Cambridge University Press},
	author = {Bekkerman, R. and Bilenko, M. and Langford, J.},
	year = {2011},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\ALLK476X\\Bekkerman et al. - Scaling Up Machine Learning Parallel and Distributed Approaches.pdf:application/pdf},
}

@incollection{rojas_backpropagation_1996,
	address = {Berlin, Heidelberg},
	title = {The {Backpropagation} {Algorithm}},
	isbn = {978-3-642-61068-4},
	url = {https://doi.org/10.1007/978-3-642-61068-4_7},
	abstract = {We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems—the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.},
	language = {en},
	urldate = {2025-08-17},
	booktitle = {Neural {Networks}: {A} {Systematic} {Introduction}},
	publisher = {Springer},
	author = {Rojas, Raúl},
	editor = {Rojas, Raúl},
	year = {1996},
	doi = {10.1007/978-3-642-61068-4_7},
	pages = {149--182},
}

@article{perez_karich_emergence_2025,
	title = {Emergence and {Evolution} of ‘{Big} {Data}’ {Research}: {A} 30-{Year} {Scientometric} {Analysis} of the {Knowledge} {Field}},
	volume = {2},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {3042-5042},
	shorttitle = {Emergence and {Evolution} of ‘{Big} {Data}’ {Research}},
	url = {https://www.mdpi.com/3042-5042/2/3/15},
	doi = {10.3390/metrics2030015},
	abstract = {In the ongoing ‘data revolution’, the ubiquity of digital data in society underlines a transformative era. This is mirrored in the sciences, where ‘big data’ has emerged as a major research field. This article significantly extends previous scientometric analyses by tracing the field’s conceptual emergence and evolution across a 30-year period (1993–2022). Bibliometric analysis is based on 17 data categories that co-constitute the conceptual network of ‘big data’ research. Using Scopus, the search query resulted in 70,163 articles and 315,235 author keywords. These are analysed aggregately regarding co-occurrences of the 17 data categories and co-occurrences of data categories with author keywords, and regarding their disciplinary distributions and interdisciplinary reach. Temporal analysis reveals two major development phases: 1993–2012 and 2013–2022. The study demonstrates: (1) the rapid expansion of the research field concentrated on seven main data categories; (2) the consolidation of keyword (co-)occurrences on ‘machine learning’, ‘deep learning’, ‘artificial intelligence’ and ‘cloud computing’; and (3) significant interdisciplinarity across four main subject areas. Scholars can use the findings to combine data categories and author keywords in ways that align scholarly work with specific thematic and disciplinary interests. The findings could also inform research funding, especially concerning opportunities for cross-disciplinary research.},
	language = {en},
	number = {3},
	urldate = {2025-08-17},
	journal = {Metrics},
	author = {Perez Karich, Ignacio and Joss, Simon},
	month = sep,
	year = {2025},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cloud computing, bibliometrics, big data, deep learning, digital data, intelligent data, machine learning, novel data, scientometrics, social media data},
	pages = {15},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\GER4LNH9\\Perez Karich and Joss - 2025 - Emergence and Evolution of ‘Big Data’ Research A 30-Year Scientometric Analysis of the Knowledge Fi.pdf:application/pdf},
}

@incollection{oneto_limitations_2020,
	address = {Cham},
	title = {Limitations of {Shallow} {Networks}},
	volume = {896},
	isbn = {978-3-030-43882-1 978-3-030-43883-8},
	url = {http://link.springer.com/10.1007/978-3-030-43883-8_6},
	abstract = {Although originally biologically inspired neural networks were introduced as multilayer computational models, shallow networks have been dominant in applications till the recent renewal of interest in deep architectures. Experimental evidence and successful applications of deep networks pose theoretical questions asking: When and why are deep networks better than shallow ones? This chapter presents some probabilistic and constructive results on limitations of shallow networks. It shows implications of geometrical properties of high-dimensional spaces for probabilistic lower bounds on network complexity. The bounds depend on covering numbers of dictionaries of computational units and sizes of domains of functions to be computed. Probabilistic results are complemented by constructive ones built using Hadamard matrices and pseudo-noise sequences.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {Recent {Trends} in {Learning} {From} {Data}},
	publisher = {Springer International Publishing},
	author = {Kůrková, Věra},
	editor = {Oneto, Luca and Navarin, Nicolò and Sperduti, Alessandro and Anguita, Davide},
	year = {2020},
	doi = {10.1007/978-3-030-43883-8_6},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {129--154},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\LRDPPLMW\\Kůrková - 2020 - Limitations of Shallow Networks.pdf:application/pdf},
}

@article{theis_end_2017,
	title = {The {End} of {Moore}'s {Law}: {A} {New} {Beginning} for {Information} {Technology}},
	volume = {19},
	issn = {1558-366X},
	shorttitle = {The {End} of {Moore}'s {Law}},
	url = {https://ieeexplore.ieee.org/abstract/document/7878935},
	doi = {10.1109/MCSE.2017.29},
	abstract = {The insights contained in Gordon Moore's now famous 1965 and 1975 papers have broadly guided the development of semiconductor electronics for over 50 years. However, the field-effect transistor is approaching some physical limits to further miniaturization, and the associated rising costs and reduced return on investment appear to be slowing the pace of development. Far from signaling an end to progress, this gradual "end of Moore's law" will open a new era in information technology as the focus of research and development shifts from miniaturization of long-established technologies to the coordinated introduction of new devices, new integration technologies, and new architectures for computing.},
	number = {2},
	urldate = {2025-08-18},
	journal = {Computing in Science \& Engineering},
	author = {Theis, Thomas N. and Wong, H.-S. Philip},
	month = mar,
	year = {2017},
	keywords = {Computer architecture, Memory management, Random access memory, Scientific computing, Algorithm design and analysis, algorithms implemented in hardware, emerging technologies, Field effect transistors, introductory and survey, memory technologies, Moore's Law, neural nets, scientific computing, Switching circuits},
	pages = {41--50},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\Z2DFSC9Y\\7878935.html:text/html},
}

@inproceedings{li_understanding_2024,
	address = {Sydney Australia},
	title = {Understanding {Communication} {Characteristics} of {Distributed} {Training}},
	isbn = {979-8-4007-1758-1},
	url = {https://dl.acm.org/doi/10.1145/3663408.3663409},
	doi = {10.1145/3663408.3663409},
	abstract = {Communication is pivotal in distributed training and a thorough understanding of its characteristics is essential for future optimizations. However, prior works are limited, either focusing on customized optimizations or conducting incomplete explorations on communication characteristics. In this work, we systematically analyze the communication characteristics of distributed training, considering two key aspects of communication: pattern and overhead, and assessing a broad spectrum of determinant factors. In particular, we extensively investigate the features of communication patterns, such as predictability, and comprehensively evaluate the impact of various factors on communication overhead. Additionally, we develop and validate an analytical formulation to estimate communication overhead, providing a mathematical understanding of models with predictability.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {Proceedings of the 8th {Asia}-{Pacific} {Workshop} on {Networking}},
	publisher = {ACM},
	author = {Li, Wenxue and Liu, Xiangzhou and Li, Yuxuan and Jin, Yilun and Tian, Han and Zhong, Zhizhen and Liu, Guyue and Zhang, Ying and Chen, Kai},
	month = aug,
	year = {2024},
	pages = {1--8},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\HV8B6SL3\\Li et al. - 2024 - Understanding Communication Characteristics of Distributed Training.pdf:application/pdf},
}

@article{moore_cramming_2006,
	title = {Cramming more components onto integrated circuits, {Reprinted} from {Electronics}, volume 38, number 8, {April} 19, 1965, pp.114 ff},
	volume = {11},
	doi = {10.1109/N-SSC.2006.4785860},
	journal = {Solid-State Circuits Newsletter, IEEE},
	author = {Moore, Gordon},
	month = oct,
	year = {2006},
	pages = {33 -- 35},
	file = {102770836-05-01-acc:C\:\\Users\\debrouwe\\Zotero\\storage\\3XG3AWCE\\102770836-05-01-acc.pdf:application/pdf},
}

@inproceedings{sevilla_compute_2022,
	title = {Compute {Trends} {Across} {Three} {Eras} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/2202.05924},
	doi = {10.1109/IJCNN55064.2022.9891914},
	abstract = {Compute, data, and algorithmic advances are the three fundamental factors that guide the progress of modern Machine Learning (ML). In this paper we study trends in the most readily quantiﬁed factor – compute. We show that before 2010 training compute grew in line with Moore’s law, doubling roughly every 20 months. Since the advent of Deep Learning in the early 2010s, the scaling of training compute has accelerated, doubling approximately every 6 months. In late 2015, a new trend emerged as ﬁrms developed large-scale ML models with 10 to 100-fold larger requirements in training compute. Based on these observations we split the history of compute in ML into three eras: the Pre Deep Learning Era , the Deep Learning Era and the Large-Scale Era . Overall, our work highlights the fast-growing compute requirements for training advanced ML systems.},
	language = {en},
	urldate = {2025-08-18},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
	month = jul,
	year = {2022},
	note = {arXiv:2202.05924 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	pages = {1--8},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\XLQX6F7X\\Sevilla et al. - 2022 - Compute Trends Across Three Eras of Machine Learning.pdf:application/pdf},
}

@inproceedings{jiang_megascale_2024,
	address = {Santa Clara, CA},
	title = {{MegaScale}: {Scaling} {Large} {Language} {Model} {Training} to {More} {Than} 10,000 {GPUs}},
	isbn = {978-1-939133-39-7},
	url = {https://www.usenix.org/conference/nsdi24/presentation/jiang-ziheng},
	booktitle = {21st {USENIX} {Symposium} on {Networked} {Systems} {Design} and {Implementation} ({NSDI} 24)},
	publisher = {USENIX Association},
	author = {Jiang, Ziheng and Lin, Haibin and Zhong, Yinmin and Huang, Qi and Chen, Yangrui and Zhang, Zhi and Peng, Yanghua and Li, Xiang and Xie, Cong and Nong, Shibiao and Jia, Yulu and He, Sun and Chen, Hongmin and Bai, Zhihao and Hou, Qi and Yan, Shipeng and Zhou, Ding and Sheng, Yiyao and Jiang, Zhuo and Xu, Haohan and Wei, Haoran and Zhang, Zhang and Nie, Pengfei and Zou, Leqi and Zhao, Sida and Xiang, Liang and Liu, Zherui and Li, Zhe and Jia, Xiaoying and Ye, Jianxi and Jin, Xin and Liu, Xin},
	month = apr,
	year = {2024},
	pages = {745--760},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\MKX3ZGWB\\Jiang et al. - MegaScale Scaling Large Language Model Training to More Than 10,000 GPUs.pdf:application/pdf},
}

@inproceedings{zhe_fan_gpu_2004,
	address = {Pittsburgh, PA, USA},
	title = {{GPU} {Cluster} for {High} {Performance} {Computing}},
	isbn = {978-0-7695-2153-4},
	url = {http://ieeexplore.ieee.org/document/1392977/},
	doi = {10.1109/SC.2004.26},
	abstract = {Inspired by the attractive Flops/dollar ratio and the incredible growth in the speed of modern graphics processing units (GPUs), we propose to use a cluster of GPUs for high performance scientiﬁc computing. As an example application, we have developed a parallel ﬂow simulation using the lattice Boltzmann model (LBM) on a GPU cluster and have simulated the dispersion of airborne contaminants in the Times Square area of New York City. Using 30 GPU nodes, our simulation can compute a 480x400x80 LBM in 0.31 second/step, a speed which is 4.6 times faster than that of our CPU cluster implementation. Besides the LBM, we also discuss other potential applications of the GPU cluster, such as cellular automata, PDE solvers, and FEM.},
	language = {en},
	urldate = {2025-08-21},
	booktitle = {Proceedings of the {ACM}/{IEEE} {SC2004} {Conference}},
	publisher = {IEEE},
	author = {{Zhe Fan} and {Feng Qiu} and Kaufman, A. and Yoakum-Stover, S.},
	year = {2004},
	pages = {47--47},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\8AQ8PBAH\\Zhe Fan et al. - 2004 - GPU Cluster for High Performance Computing.pdf:application/pdf},
}

@article{xing_strategies_2016,
	title = {Strategies and {Principles} of {Distributed} {Machine} {Learning} on {Big} {Data}},
	volume = {2},
	issn = {2095-8099},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809916309468},
	doi = {10.1016/J.ENG.2016.02.008},
	abstract = {The rise of big data has led to new demands for machine learning (ML) systems to learn complex models, with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics (such as high-dimensional latent features, intermediate representations, and decision functions) thereupon. In order to run ML algorithms at such scales, on a distributed cluster with tens to thousands of machines, it is often the case that significant engineering efforts are required—and one might fairly ask whether such engineering truly falls within the domain of ML research. Taking the view that “big” ML systems can benefit greatly from ML-rooted statistical and algorithmic insights—and that ML researchers should therefore not shy away from such systems design—we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of big ML systems and architectures, with the goal of understanding how to make them efficient, generally applicable, and supported with convergence and scaling guarantees. They concern four key questions that traditionally receive little attention in ML research: How can an ML program be distributed over a cluster? How can ML computation be bridged with inter-machine communication? How can such communication be performed? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and enlarge the area that lies between ML and systems.},
	number = {2},
	urldate = {2025-08-21},
	journal = {Engineering},
	author = {Xing, Eric P. and Ho, Qirong and Xie, Pengtao and Wei, Dai},
	month = jun,
	year = {2016},
	keywords = {Distributed systems, Machine learning, Artificial intelligence big data, Big model, Data-parallelism, Model-parallelism, Principles, Theory},
	pages = {179--195},
	file = {ScienceDirect Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\F2DQSRD4\\S2095809916309468.html:text/html;Submitted Version:C\:\\Users\\debrouwe\\Zotero\\storage\\XMRJTBJP\\Xing et al. - 2016 - Strategies and Principles of Distributed Machine Learning on Big Data.pdf:application/pdf},
}

@misc{dong_towards_2024,
	title = {Towards {Low}-bit {Communication} for {Tensor} {Parallel} {LLM} {Inference}},
	url = {http://arxiv.org/abs/2411.07942},
	doi = {10.48550/arXiv.2411.07942},
	abstract = {Tensor parallelism provides an effective way to increase server large language model (LLM) inference efficiency despite adding an additional communication cost. However, as server LLMs continue to scale in size, they will need to be distributed across more devices, magnifying the communication cost. One way to approach this problem is with quantization, but current methods for LLMs tend to avoid quantizing the features that tensor parallelism needs to communicate. Taking advantage of consistent outliers in communicated features, we introduce a quantization method that reduces communicated values on average from 16 bits to 4.2 bits while preserving nearly all of the original performance. For instance, our method maintains around 98.0\% and 99.5\% of Gemma 2 27B's and Llama 2 13B's original performance, respectively, averaged across all tasks we evaluated on.},
	urldate = {2025-08-21},
	publisher = {arXiv},
	author = {Dong, Harry and Johnson, Tyler and Cho, Minsik and Soroush, Emad},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07942 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\UBXZXVZ7\\Dong et al. - 2024 - Towards Low-bit Communication for Tensor Parallel LLM Inference.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\X58U5CVT\\2411.html:text/html},
}

@inproceedings{lian_asynchronous_2018,
	title = {Asynchronous {Decentralized} {Parallel} {Stochastic} {Gradient} {Descent}},
	url = {https://proceedings.mlr.press/v80/lian18a.html},
	abstract = {Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal O(1/K−−√)O(1/K)O(1/{\textbackslash}sqrt\{K\}) rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8x faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.},
	language = {en},
	urldate = {2025-08-21},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3043--3052},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\W6CCHJBH\\Lian et al. - 2018 - Asynchronous Decentralized Parallel Stochastic Gradient Descent.pdf:application/pdf;Supplementary PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\C4YSLYPS\\Lian et al. - 2018 - Asynchronous Decentralized Parallel Stochastic Gradient Descent.pdf:application/pdf},
}

@misc{ratnasamy_collective_2025,
	address = {Berkeley, CA},
	type = {Lecture},
	title = {Collective {Operations}},
	url = {https://textbook.cs168.io/beyond-client-server/collective-operations.html},
	language = {en},
	urldate = {2025-08-21},
	author = {Ratnasamy, : Sylvia and Shakir, Rob and Peyrin, Kao},
	year = {2025},
	file = {[CS168 SP25] Lecture 25 - Collectives (1) (1):C\:\\Users\\debrouwe\\Zotero\\storage\\KWE5PWEH\\[CS168 SP25] Lecture 25 - Collectives (1) (1).pdf:application/pdf},
}

@article{yang_balancing_2025,
	title = {Balancing communication overhead and accuracy in compression integration: a survey},
	volume = {81},
	issn = {1573-0484},
	shorttitle = {Balancing communication overhead and accuracy in compression integration},
	url = {https://doi.org/10.1007/s11227-025-07451-z},
	doi = {10.1007/s11227-025-07451-z},
	abstract = {In large-scale distributed training, communication compression techniques are widely used to reduce the significant communication overhead caused by the frequent exchange of model parameters or gradients between training nodes. However, these techniques often introduce additional computational complexity and may lead to data loss, thereby affecting model convergence and performance. This review examines key optimization methods in communication compression, including pruning techniques that remove irrelevant weights, quantization techniques that convert floating-point parameters to low-precision representations, and sparsification techniques that transmit only critical gradients. Low-rank approximation techniques, which compress parameters through matrix factorization, are particularly useful for large-scale models. These techniques have also been widely applied in various application scenarios, demonstrating their effectiveness in different environments. Application scenarios include distributed training, federated learning, and edge computing, where bottlenecks are carefully identified and evaluated in common scenarios, providing a basis for further optimization. Future development directions emphasize co-design of hardware and algorithms, dynamic strategies, and cross-layer optimization. This study provides valuable comparisons of key methods and theoretical analysis for efficient communication compression in distributed systems.},
	language = {en},
	number = {8},
	urldate = {2025-08-23},
	journal = {The Journal of Supercomputing},
	author = {Yang, Aiqiang and Liu, Jie and Yang, Bo and Mo, Zeyao and Li, Keqin},
	month = jun,
	year = {2025},
	keywords = {Collective communication, Communication compression, Distributed training, Stochastic gradient},
	pages = {964},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\S9GEHVKU\\Yang et al. - 2025 - Balancing communication overhead and accuracy in compression integration a survey.pdf:application/pdf},
}

@inproceedings{rashidi_astra-sim_2020,
	title = {{ASTRA}-{SIM}: {Enabling} {SW}/{HW} {Co}-{Design} {Exploration} for {Distributed} {DL} {Training} {Platforms}},
	shorttitle = {{ASTRA}-{SIM}},
	url = {https://ieeexplore.ieee.org/document/9238637},
	doi = {10.1109/ISPASS48437.2020.00018},
	abstract = {Modern Deep Learning systems heavily rely on distributed training over high-performance accelerator (e.g., TPU, GPU)-based hardware platforms. Examples today include Google's Cloud TPU and Facebook's Zion. DNN training involves a complex interplay between the DNN model architecture, paral-lelization strategy, scheduling strategy, collective communication algorithm, network topology, and the end-point accelerator. As innovation in AI/ML models continues to grow at an accelerated rate, there is a need for a comprehensive methodology to understand and navigate this complex SW/HW design-space for future systems to support efficient training of future DNN models. In this work, we make the following contributions (i) establish the SW/HW design-space for Distributed Training over a hierarchical scale-up fabric, (ii) develop a network simulator for navigating the design-space, and (iii) demonstrate the promise of algorithm-topology co-design for speeding up end to end training.},
	urldate = {2025-08-23},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	author = {Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar},
	month = aug,
	year = {2020},
	keywords = {Training, Software, Software algorithms, Scheduling, Network topology, Collective communication, Distributed training, High performance training systems, Navigation, Technological innovation, Training parallelism},
	pages = {81--92},
	file = {astrasim_ispass2020:C\:\\Users\\debrouwe\\Zotero\\storage\\925J4YRK\\astrasim_ispass2020.pdf:application/pdf;PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\EV2YBRD5\\Rashidi et al. - 2020 - ASTRA-SIM Enabling SWHW Co-Design Exploration for Distributed DL Training Platforms.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\BMD3A6ZP\\9238637.html:text/html},
}

@inproceedings{won_astra-sim20_2023,
	title = {{ASTRA}-sim2.0: {Modeling} {Hierarchical} {Networks} and {Disaggregated} {Systems} for {Large}-model {Training} at {Scale}},
	shorttitle = {{ASTRA}-sim2.0},
	url = {http://arxiv.org/abs/2303.14006},
	doi = {10.1109/ISPASS57527.2023.00035},
	abstract = {As deep learning models and input data are scaling at an unprecedented rate, it is inevitable to move towards distributed training platforms to fit the model and increase training throughput. State-of-the-art approaches and techniques, such as wafer-scale nodes, multi-dimensional network topologies, disaggregated memory systems, and parallelization strategies, have been actively adopted by emerging distributed training systems. This results in a complex SW/HW co-design stack of distributed training, necessitating a modeling/simulation infrastructure for design-space exploration. In this paper, we extend the open-source ASTRA-sim infrastructure and endow it with the capabilities to model state-of-the-art and emerging distributed training models and platforms. More specifically, (i) we enable ASTRA-sim to support arbitrary model parallelization strategies via a graph-based training-loop implementation, (ii) we implement a parameterizable multi-dimensional heterogeneous topology generation infrastructure with analytical performance estimates enabling simulating target systems at scale, and (iii) we enhance the memory system modeling to support accurate modeling of in-network collective communication and disaggregated memory systems. With such capabilities, we run comprehensive case studies targeting emerging distributed models and platforms. This infrastructure lets system designers swiftly traverse the complex co-design stack and give meaningful insights when designing and deploying distributed training platforms at scale.},
	urldate = {2025-08-23},
	booktitle = {2023 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	author = {Won, William and Heo, Taekyung and Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar},
	month = apr,
	year = {2023},
	note = {arXiv:2303.14006 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	pages = {283--294},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\NJM3WWIH\\Won et al. - 2023 - ASTRA-sim2.0 Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at S.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\3CPX6RG5\\2303.html:text/html},
}

@inproceedings{agarwal_garnet_2009,
	title = {{GARNET}: {A} detailed on-chip network model inside a full-system simulator},
	booktitle = {Performance {Analysis} of {Systems} and {Software}, 2009. {ISPASS} 2009. {IEEE} {International} {Symposium} on},
	publisher = {IEEE},
	author = {Agarwal, Niket and Krishna, Tushar and Peh, Li-Shiuan and Jha, Niraj K},
	year = {2009},
	pages = {33--42},
}

@misc{samajdar_scale-sim_2019,
	title = {{SCALE}-{Sim}: {Systolic} {CNN} {Accelerator} {Simulator}},
	shorttitle = {{SCALE}-{Sim}},
	url = {http://arxiv.org/abs/1811.02883},
	doi = {10.48550/arXiv.1811.02883},
	abstract = {Systolic Arrays are one of the most popular compute substrates within Deep Learning accelerators today, as they provide extremely high efficiency for running dense matrix multiplications. However, the research community lacks tools to insights on both the design trade-offs and efficient mapping strategies for systolic-array based accelerators. We introduce Systolic CNN Accelerator Simulator (SCALE-Sim), which is a configurable systolic array based cycle accurate DNN accelerator simulator. SCALE-Sim exposes various micro-architectural features as well as system integration parameters to the designer to enable comprehensive design space exploration. This is the first systolic-array simulator tuned for running DNNs to the best of our knowledge. Using SCALE-Sim, we conduct a suite of case studies and demonstrate the effect of bandwidth, data flow and aspect ratio on the overall runtime and energy of Deep Learning kernels across vision, speech, text, and games. We believe that these insights will be highly beneficial to architects and ML practitioners.},
	urldate = {2025-08-23},
	publisher = {arXiv},
	author = {Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
	month = feb,
	year = {2019},
	note = {arXiv:1811.02883 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\INUQQQWS\\Samajdar et al. - 2019 - SCALE-Sim Systolic CNN Accelerator Simulator.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\UPWD3JQT\\1811.html:text/html},
}

@misc{sridharan_chakra_2023,
	title = {Chakra: {Advancing} {Performance} {Benchmarking} and {Co}-design using {Standardized} {Execution} {Traces}},
	shorttitle = {Chakra},
	url = {http://arxiv.org/abs/2305.14516},
	doi = {10.48550/arXiv.2305.14516},
	abstract = {Benchmarking and co-design are essential for driving optimizations and innovation around ML models, ML software, and next-generation hardware. Full workload benchmarks, e.g. MLPerf, play an essential role in enabling fair comparison across different software and hardware stacks especially once systems are fully designed and deployed. However, the pace of AI innovation demands a more agile methodology to benchmark creation and usage by simulators and emulators for future system co-design. We propose Chakra, an open graph schema for standardizing workload specification capturing key operations and dependencies, also known as Execution Trace (ET). In addition, we propose a complementary set of tools/capabilities to enable collection, generation, and adoption of Chakra ETs by a wide range of simulators, emulators, and benchmarks. For instance, we use generative AI models to learn latent statistical properties across thousands of Chakra ETs and use these models to synthesize Chakra ETs. These synthetic ETs can obfuscate key proprietary information and also target future what-if scenarios. As an example, we demonstrate an end-to-end proof-of-concept that converts PyTorch ETs to Chakra ETs and uses this to drive an open-source training system simulator (ASTRA-sim). Our end-goal is to build a vibrant industry-wide ecosystem of agile benchmarks and tools to drive future AI system co-design.},
	urldate = {2025-08-23},
	publisher = {arXiv},
	author = {Sridharan, Srinivas and Heo, Taekyung and Feng, Louis and Wang, Zhaodong and Bergeron, Matt and Fu, Wenyin and Zheng, Shengbao and Coutinho, Brian and Rashidi, Saeed and Man, Changhai and Krishna, Tushar},
	month = may,
	year = {2023},
	note = {arXiv:2305.14516 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\YI4CVP8X\\Sridharan et al. - 2023 - Chakra Advancing Performance Benchmarking and Co-design using Standardized Execution Traces.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\VLJYGWIU\\2305.html:text/html},
}

@incollection{riley_ns-3_2010,
	address = {Berlin, Heidelberg},
	title = {The ns-3 {Network} {Simulator}},
	isbn = {978-3-642-12331-3},
	url = {https://doi.org/10.1007/978-3-642-12331-3_2},
	abstract = {As networks of computing devices grow larger and more complex, the need for highly accurate and scalable network simulation technologies becomes critical. Despite the emergence of large-scale testbeds for network research, simulation still plays a vital role in terms of scalability (both in size and in experimental speed), reproducibility, rapid prototyping, and education. With simulation based studies, the approach can be studied in detail at varying scales, with varying data applications, varying field conditions, and will result in reproducible and analyzable results.},
	language = {en},
	urldate = {2025-08-23},
	booktitle = {Modeling and {Tools} for {Network} {Simulation}},
	publisher = {Springer},
	author = {Riley, George F. and Henderson, Thomas R.},
	editor = {Wehrle, Klaus and Güneş, Mesut and Gross, James},
	year = {2010},
	doi = {10.1007/978-3-642-12331-3_2},
	pages = {15--34},
}

@misc{yoo_towards_2025,
	title = {Towards {Easy} and {Realistic} {Network} {Infrastructure} {Testing} for {Large}-scale {Machine} {Learning}},
	url = {http://arxiv.org/abs/2504.20854},
	doi = {10.48550/arXiv.2504.20854},
	abstract = {This paper lays the foundation for Genie, a testing framework that captures the impact of real hardware network behavior on ML workload performance, without requiring expensive GPUs. Genie uses CPU-initiated traffic over a hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim simulator to model interaction between the network and the ML workload.},
	urldate = {2025-08-23},
	publisher = {arXiv},
	author = {Yoo, Jinsun and Lao, ChonLam and Cao, Lianjie and Lantz, Bob and Yu, Minlan and Krishna, Tushar and Sharma, Puneet},
	month = apr,
	year = {2025},
	note = {arXiv:2504.20854 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence, Computer Science - Networking and Internet Architecture, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Presented as a poster in NSDI 25},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\7VB3TG95\\Yoo et al. - 2025 - Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\DWSEJSXM\\2504.html:text/html},
}

@misc{yoo_towards_2025-1,
	title = {Towards {Easy} and {Realistic} {Network} {Infrastructure} {Testing} for {Large}-scale {Machine} {Learning}},
	url = {http://arxiv.org/abs/2504.20854},
	doi = {10.48550/arXiv.2504.20854},
	abstract = {This paper lays the foundation for Genie, a testing framework that captures the impact of real hardware network behavior on ML workload performance, without requiring expensive GPUs. Genie uses CPU-initiated traffic over a hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim simulator to model interaction between the network and the ML workload.},
	urldate = {2025-08-23},
	publisher = {arXiv},
	author = {Yoo, Jinsun and Lao, ChonLam and Cao, Lianjie and Lantz, Bob and Yu, Minlan and Krishna, Tushar and Sharma, Puneet},
	month = apr,
	year = {2025},
	note = {arXiv:2504.20854 [cs]
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence, Computer Science - Networking and Internet Architecture, Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Presented as a poster in NSDI 25},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\6SHXQTQ2\\Yoo et al. - 2025 - Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\JLTZ2UJI\\2504.html:text/html},
}

@misc{besta_evalnet_2025,
	title = {{EvalNet}: {A} {Practical} {Toolchain} for {Generation} and {Analysis} of {Extreme}-{Scale} {Interconnects}},
	shorttitle = {{EvalNet}},
	url = {http://arxiv.org/abs/2105.12663},
	doi = {10.48550/arXiv.2105.12663},
	abstract = {The diversity of communication paths in a network - especially non-minimal paths - is a key enabler of performance at extreme scales. We present EvalNet, a toolchain for scalable generation and analysis over 25 important network topologies, such as Slim Fly, PolarFly, and Orthogonal Fat Trees, with a strong focus on path diversity metrics. EvalNet provides an extensive and fine-grained analysis of shortest and non-shortest paths, including their multiplicities, lengths, and interference. It supports exact measurement and visualization of bandwidth and throughput between every router pair, enabling unprecedented insight into routing potential. EvalNet also includes detailed models for construction cost and power consumption, and interfaces seamlessly with established simulators, which we tune to support large-scale evaluations on low-cost hardware. Using EvalNet, we deliver the widest and most comprehensive path diversity study to date, demonstrating how path diversity underpins throughput and scalability, and facilitating progress towards new frontiers in extreme-scale network design.},
	urldate = {2025-08-23},
	publisher = {arXiv},
	author = {Besta, Maciej and Iff, Patrick and Schneider, Marcel and Blach, Nils and Maissen, Alessandro and Girolamo, Salvatore Di and Domke, Jens and Krattenmacher, Jascha and Singla, Ankit and Lakhotia, Kartik and Monroe, Laura and Petrini, Fabrizio and Gerstenberger, Robert and Hoefler, Torsten},
	month = jun,
	year = {2025},
	note = {arXiv:2105.12663 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance, Computer Science - Networking and Internet Architecture},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\ILJ9LRZA\\Besta et al. - 2025 - EvalNet A Practical Toolchain for Generation and Analysis of Extreme-Scale Interconnects.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\AAT7DKRE\\2105.html:text/html},
}

@misc{besta_towards_2021,
	title = {Towards {Million}-{Server} {Network} {Simulations} on {Just} a {Laptop}},
	url = {http://arxiv.org/abs/2105.12663},
	doi = {10.48550/arXiv.2105.12663},
	abstract = {The growing size of data center and HPC networks pose unprecedented requirements on the scalability of simulation infrastructure. The ability to simulate such large-scale interconnects on a simple PC would facilitate research efforts. Unfortunately, as we first show in this work, existing shared-memory packet-level simulators do not scale to the sizes of the largest networks considered today. We then illustrate a feasibility analysis and a set of enhancements that enable a simple packet-level htsim simulator to scale to the unprecedented simulation sizes on a single PC. Our code is available online and can be used to design novel schemes in the coming era of omnipresent data centers and HPC clusters.},
	urldate = {2025-08-23},
	publisher = {arXiv},
	author = {Besta, Maciej and Schneider, Marcel and Girolamo, Salvatore Di and Singla, Ankit and Hoefler, Torsten},
	month = may,
	year = {2021},
	note = {arXiv:2105.12663 [cs]
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance, Computer Science - Networking and Internet Architecture},
	file = {Preprint PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\X6RWA3I6\\Besta et al. - 2021 - Towards Million-Server Network Simulations on Just a Laptop.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\IKZ24CHP\\2105.html:text/html},
}

@book{norman_design_2013,
	address = {Cambridge (Mass.)},
	edition = {Rev. and expanded edition},
	title = {The design of everyday things},
	isbn = {978-0-465-05065-9},
	abstract = {"Even the smartest among us can feel inept as we fail to figure out which light switch or oven burner to turn on, or whether to push, pull, or slide a door. The fault, argues this ingenious-even liberating-book, lies not in ourselves, but in product design that ignores the needs of users and the principles of cognitive psychology. The problems range from ambiguous and hidden controls to arbitrary relationships between controls and functions, coupled with a lack of feedback or other assistance and unreasonable demands on memorization. The Design of Everyday Things shows that good, usable design is possible. The rules are simple : make things visible, exploit natural relationships that couple function and control, and make intelligent use of constraints. The goal : guide the user effortlessly to the right action on the right control at the right time. In this entertaining and insightful analysis, cognitive scientist Don Norman hails excellence of design as the most important key to regaining the competitive edge in influencing consumer behavior. Now fully expanded and updated, with a new introduction by the author, The Design of Everyday Things is a powerful primer on how-and why-some products satisfy customers while others only frustrate them."},
	language = {en},
	publisher = {MIT press},
	author = {Norman, Donald A.},
	year = {2013},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\4PP28HZS\\Norman - 2013 - The design of everyday things.pdf:application/pdf},
}

@inproceedings{sinha_human_2010,
	title = {Human {Computer} {Interaction}},
	url = {https://ieeexplore.ieee.org/abstract/document/5698279},
	doi = {10.1109/ICETET.2010.85},
	abstract = {This paper intends to provide an overview on the subject of Human Computer Interaction (HCI), also sometimes referred as Man Machine Interaction or MMI. We will be discussing about basic definition and a brief history about the origin of HCI. In the later sections, discussions will be made on existing technologies in HCI and their developments. In the end, we also intend to discuss about the future directions in HCI.},
	urldate = {2025-08-23},
	booktitle = {2010 3rd {International} {Conference} on {Emerging} {Trends} in {Engineering} and {Technology}},
	author = {Sinha, Gaurav and Shahi, Rahul and Shankar, Mani},
	month = nov,
	year = {2010},
	note = {ISSN: 2157-0485},
	keywords = {Educational institutions, Computers, History, Human computer interaction, Humans, Service robots},
	pages = {1--4},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\X5FVDA5D\\Sinha et al. - 2010 - Human Computer Interaction.pdf:application/pdf},
}

@article{goldstine_electronic_1996,
	title = {The {Electronic} {Numerical} {Integrator} and {Computer} ({ENIAC})},
	volume = {18},
	issn = {1934-1547},
	url = {https://ieeexplore.ieee.org/document/476557},
	doi = {10.1109/85.476557},
	abstract = {This paper was first published in Mathematical Tables and Other Aids to Computation just after the ENIAC was announced in 1946. It was the major source of technical information about the machine for the scientific world of the time. Even today it ranks as one of the classic descriptions of the ENIAC. This paper is reprinted by the kind permission of the American Mathematical Society and the National Academy of Sciences.},
	number = {1},
	urldate = {2025-08-23},
	journal = {IEEE Annals of the History of Computing},
	author = {Goldstine, H.H. and Goldstine, A.},
	year = {1996},
	keywords = {Switches, Voltage, Computer aided instruction, Testing, Transmitters, Turning},
	pages = {10--16},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\PMSJNCUK\\Goldstine and Goldstine - 1996 - The Electronic Numerical Integrator and Computer (ENIAC).pdf:application/pdf},
}

@article{kosch_survey_2023,
	title = {A {Survey} on {Measuring} {Cognitive} {Workload} in {Human}-{Computer} {Interaction}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3582272},
	doi = {10.1145/3582272},
	abstract = {The ever-increasing number of computing devices around us results in more and more systems competing for our attention, making cognitive workload a crucial factor for the user experience of human-computer interfaces. Research in
              Human-Computer Interaction (HCI)
              has used various metrics to determine users’ mental demands. However, there needs to be a systematic way to choose an appropriate and effective measure for cognitive workload in experimental setups, posing a challenge to their reproducibility. We present a literature survey of past and current metrics for cognitive workload used throughout HCI literature to address this challenge. By initially exploring what cognitive workload resembles in the HCI context, we derive a categorization supporting researchers and practitioners in selecting cognitive workload metrics for system design and evaluation. We conclude with three following research gaps: (1) defining and interpreting cognitive workload in HCI, (2) the hidden cost of the NASA-TLX, and (3) HCI research as a catalyst for workload-aware systems, highlighting that HCI research has to deepen and conceptualize the understanding of cognitive workload in the context of interactive computing systems.},
	language = {en},
	number = {13s},
	urldate = {2025-08-23},
	journal = {ACM Computing Surveys},
	author = {Kosch, Thomas and Karolus, Jakob and Zagermann, Johannes and Reiterer, Harald and Schmidt, Albrecht and Woźniak, Paweł W.},
	month = dec,
	year = {2023},
	pages = {1--39},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\I2GTFWS8\\Kosch et al. - 2023 - A Survey on Measuring Cognitive Workload in Human-Computer Interaction.pdf:application/pdf},
}

@inproceedings{perez-quinones_collaborative_1996,
	address = {New York, NY, USA},
	series = {{CHI} '96},
	title = {A collaborative model of feedback in human-computer interaction},
	isbn = {978-0-89791-777-3},
	url = {https://dl.acm.org/doi/10.1145/238386.238535},
	doi = {10.1145/238386.238535},
	urldate = {2025-08-23},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pérez-Quiñones, Manuel A. and Sibert, John L.},
	month = apr,
	year = {1996},
	pages = {316--323},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\ZYP7CR7B\\Pérez-Quiñones and Sibert - 1996 - A collaborative model of feedback in human-computer interaction.pdf:application/pdf},
}

@misc{clark_grounding_1991,
	title = {Grounding in communication.},
	url = {https://awspntest.apa.org/record/1991-98452-006},
	urldate = {2025-08-24},
	author = {Clark, Herbert},
	year = {1991},
	file = {Grounding in communication.:C\:\\Users\\debrouwe\\Zotero\\storage\\NXI8WD2C\\1991-98452-006.html:text/html},
}

@article{hamidli_introduction_2023,
	title = {Introduction to {UI}/{UX} {Design}: {Key} {Concepts} and {Principles}},
	shorttitle = {Introduction to {UI}/{UX} {Design}},
	url = {https://www.academia.edu/98036432/Introduction_to_UI_UX_Design_Key_Concepts_and_Principles},
	abstract = {This article provides an overview of the key concepts and principles of UI/UX design. It begins by defining UI/UX design and discussing its importance in creating effective and engaging digital products. The article then outlines the key concepts and},
	urldate = {2025-08-24},
	author = {Hamidli, Nesi},
	month = mar,
	year = {2023},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\4WB6W9HX\\Hamidli - Introduction to UIUX Design Key Concepts and Principles.pdf:application/pdf;Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\4Z4DALIF\\Introduction_to_UI_UX_Design_Key_Concepts_and_Principles.html:text/html},
}

@inproceedings{sampath_accessibility_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Accessibility of {Command} {Line} {Interfaces}},
	isbn = {978-1-4503-8096-6},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445544},
	doi = {10.1145/3411764.3445544},
	abstract = {Command-line interfaces (CLIs) remain a popular tool among developers and system administrators. Since CLIs are text-based interfaces, they are sometimes considered accessible alternatives to predominantly visual developer tools like IDEs. However, there is no systematic evaluation of the accessibility of CLIs in the literature. In this paper, we describe two studies with 12 developers on their experience of using CLIs with screen readers. Our findings show that CLIs have their own set of accessibility issues - the most important being CLIs are unstructured text interfaces. Based on our results, we provide a set of recommendations for improving the accessibility of command-line interfaces.},
	urldate = {2025-08-23},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Sampath, Harini and Merrick, Alice and Macvean, Andrew},
	month = may,
	year = {2021},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\8HZ4IQDM\\Sampath et al. - 2021 - Accessibility of Command Line Interfaces.pdf:application/pdf},
}

@article{billestrup_persona_2014,
	title = {Persona {Usage} in {Software} {Development}: {Advantages} and {Obstacles}},
	abstract = {The Personas technique has been promoted as a strong tool for providing software developers with a better understanding of the prospective users of their software. This paper reports from a questionnaire survey regarding knowledge about Personas and their usage in software development companies. The questionnaire survey was conducted in a limited geographical area to establish the extent of Personas usage within all companies in the chosen region and determine whether software development companies used Personas during the development process. Several issues were identified as reasons for either not using the technique or for poor application of it. The study showed that 55\% of the respondents had never heard about Personas. Among those who had heard about the Personas technique, the following obstacles towards usage of the technique were identified: Lack of knowledge of the technique, lack of resources (time and funding), Sparse descriptions – when applied and Personas not being integrated in the development.},
	language = {en},
	author = {Billestrup, Jane and Stage, Jan and Nielsen, Lene and Hansen, Kira S},
	year = {2014},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\WP9C25BH\\Billestrup et al. - 2014 - Persona Usage in Software Development Advantages and Obstacles.pdf:application/pdf},
}

@inproceedings{nyambi_persona_2014,
	title = {Persona mapping for usability of {ICTD} services},
	url = {https://ieeexplore.ieee.org/abstract/document/7068128},
	doi = {10.1109/ICASTECH.2014.7068128},
	abstract = {Information and Communication Technologies (ICTs) have been at the core of the 21st century human development efforts. The role of ICT for human development has been acknowledged and emphasized globally by entities such as the United Nations — through the World Summit on the Information Society (WSIS). Consequently a lot of ICT based services have been developed, in particular within the context of ICT for Development (ICTD) to facilitate socio-economic development of marginalized communities. However, the information embedded in technology is useless if there are limitations in accessing this information. These limitations are typically and largely due to the poor and low usability of these services, hence the efforts to explore user experience (UX) with ICTs to ensure usability of products and services. This research is premised on the idea that increased usability of ICTD services would improve the effectiveness of these efforts for the benefit of the plethora of marginalized communities. To that end, this paper initially undertakes a persona profiling to determine the key characteristics of the user communities, clustered according to the key UX attributes. Subsequently the paper carries out a detailed usability evaluation, using System Usability Scale (SUS) to determine the usability of various UI components/techniques per identified persona.},
	urldate = {2025-08-24},
	booktitle = {2014 {IEEE} 6th {International} {Conference} on {Adaptive} {Science} \& {Technology} ({ICAST})},
	author = {Nyambi, Pride Bongiwe and Telkom, Mamello Thinyane},
	month = oct,
	year = {2014},
	note = {ISSN: 2326-9448},
	keywords = {Communities, Human computer interaction, Context, Cultural differences, HCI, ICTD services, M-commerce, Mobile communication, Persona, Usability, User Experience},
	pages = {1--7},
	file = {Full Text PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\HSP44VY2\\Nyambi and Telkom - 2014 - Persona mapping for usability of ICTD services.pdf:application/pdf},
}

@inproceedings{pruitt_personas_2003,
	address = {San Francisco California},
	title = {Personas: practice and theory},
	isbn = {978-1-58113-728-6},
	shorttitle = {Personas},
	url = {https://dl.acm.org/doi/10.1145/997078.997089},
	doi = {10.1145/997078.997089},
	language = {en},
	urldate = {2025-08-24},
	booktitle = {Proceedings of the 2003 conference on {Designing} for user experiences},
	publisher = {ACM},
	author = {Pruitt, John and Grudin, Jonathan},
	month = jun,
	year = {2003},
	pages = {1--15},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\UX5TUMJJ\\Pruitt and Grudin - 2003 - Personas practice and theory.pdf:application/pdf},
}

@incollection{gobo_scientists_2022,
	address = {Cham},
	title = {Scientists, {Experts} and {Public} {Opinion}},
	isbn = {978-3-031-08306-8},
	url = {https://doi.org/10.1007/978-3-031-08306-8_8},
	abstract = {“Experts argue that..” these few words are enough to induce the reader to attribute some credibility to a claim. But who is an expert? And what is expertise? Empirical studies in STS show that expertise is a status attributed to a group of people of whom “the expert” is part. These studies also reveal that scientists with different backgrounds have different kinds of expertise and also that there are other ways of knowing that, in specific contexts, can have a pragmatic value. In decision-making processes, therefore, it is important to include as many perspectives as possible. Communication—within and without the scientific community—becomes of paramount importance. The second part of the chapter discusses science communication and its many styles: from the so-called “information deficit model” to contemporary ways of public engagement and citizen science initiatives.},
	language = {en},
	urldate = {2025-08-24},
	booktitle = {Science, {Technology} and {Society}: {An} {Introduction}},
	publisher = {Springer International Publishing},
	author = {Gobo, Giampietro and Marcheselli, Valentina},
	editor = {Gobo, Giampietro and Marcheselli, Valentina},
	year = {2022},
	doi = {10.1007/978-3-031-08306-8_8},
	pages = {163--178},
}

@incollection{gobo_scientists_2022-1,
	address = {Cham},
	title = {Scientists, {Experts} and {Public} {Opinion}},
	isbn = {978-3-031-08306-8},
	url = {https://doi.org/10.1007/978-3-031-08306-8_8},
	abstract = {“Experts argue that..” these few words are enough to induce the reader to attribute some credibility to a claim. But who is an expert? And what is expertise? Empirical studies in STS show that expertise is a status attributed to a group of people of whom “the expert” is part. These studies also reveal that scientists with different backgrounds have different kinds of expertise and also that there are other ways of knowing that, in specific contexts, can have a pragmatic value. In decision-making processes, therefore, it is important to include as many perspectives as possible. Communication—within and without the scientific community—becomes of paramount importance. The second part of the chapter discusses science communication and its many styles: from the so-called “information deficit model” to contemporary ways of public engagement and citizen science initiatives.},
	booktitle = {Science, {Technology} and {Society}: {An} {Introduction}},
	publisher = {Springer International Publishing},
	author = {Gobo, Giampietro and Marcheselli, Valentina},
	year = {2022},
	doi = {10.1007/978-3-031-08306-8_8},
	pages = {163--178},
}

@misc{nielsen_differences_2006,
	title = {Differences {Between} {Novice} and {Expert} {Users}},
	url = {https://www.nngroup.com/articles/novice-vs-expert-users/},
	author = {Nielsen, Jakob},
	year = {2006},
	annote = {Accessed: 2025-08-23},
}

@misc{noauthor_torchrec_nodate,
	title = {{TorchRec} {High} {Level} {Architecture} — {TorchRec} 1.0.0 documentation},
	url = {https://docs.pytorch.org/torchrec/high-level-arch.html},
	urldate = {2025-08-24},
	file = {TorchRec High Level Architecture — TorchRec 1.0.0 documentation:C\:\\Users\\debrouwe\\Zotero\\storage\\NRUPDZB8\\high-level-arch.html:text/html},
}

@misc{network_how_2024,
	title = {How {Much} {GPU} {Memory} is {Required} to {Run} a {Large} {Language} {Model}?},
	url = {https://blog.spheron.network/how-much-gpu-memory-is-required-to-run-a-large-language-model-find-out-here},
	abstract = {With the growing importance of LLMs in AI-driven applications, developers and companies are deploying models like GPT-4, LLaMA, and OPT-175B in real-world},
	language = {en},
	urldate = {2025-08-24},
	journal = {Spheron's Blog},
	author = {Network, Spheron},
	month = sep,
	year = {2024},
	file = {Snapshot:C\:\\Users\\debrouwe\\Zotero\\storage\\6Y79IU6D\\how-much-gpu-memory-is-required-to-run-a-large-language-model-find-out-here.html:text/html},
}

@inproceedings{gangidi_rdma_2024,
	address = {Sydney NSW Australia},
	title = {{RDMA} over {Ethernet} for {Distributed} {Training} at {Meta} {Scale}},
	isbn = {979-8-4007-0614-1},
	url = {https://dl.acm.org/doi/10.1145/3651890.3672233},
	doi = {10.1145/3651890.3672233},
	abstract = {The rapid growth in both computational density and scale in AI models in recent years motivates the construction of an efficient and reliable dedicated network infrastructure. This paper presents the design, implementation, and operation of Meta’s Remote Direct Memory Access over Converged Ethernet (RoCE) networks for distributed AI training.},
	language = {en},
	urldate = {2025-08-24},
	booktitle = {Proceedings of the {ACM} {SIGCOMM} 2024 {Conference}},
	publisher = {ACM},
	author = {Gangidi, Adithya and Miao, Rui and Zheng, Shengbao and Bondu, Sai Jayesh and Goes, Guilherme and Morsy, Hany and Puri, Rohit and Riftadi, Mohammad and Shetty, Ashmitha Jeevaraj and Yang, Jingyi and Zhang, Shuqiang and Fernandez, Mikel Jimenez and Gandham, Shashidhar and Zeng, Hongyi},
	month = aug,
	year = {2024},
	pages = {57--70},
	file = {PDF:C\:\\Users\\debrouwe\\Zotero\\storage\\KW9K7E9K\\Gangidi et al. - 2024 - RDMA over Ethernet for Distributed Training at Meta Scale.pdf:application/pdf},
}

@misc{noauthor_study_nodate,
	title = {A {Study} on {Distributed} {Strategies} for {Deep} {Learning} {Applications} in {GPU} {Clusters}},
	url = {https://arxiv.org/html/2505.12832v1},
	urldate = {2025-08-24},
	file = {A Study on Distributed Strategies for Deep Learning Applications in GPU Clusters:C\:\\Users\\debrouwe\\Zotero\\storage\\5U7NIAP5\\2505.html:text/html},
}
