{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5b5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CostModel] Adding (inter-Node, Link) bandwidth: 25, radix: 1, count: 64 (added cost: $6400)\n",
      "[CostModel] Adding (inter-Node, Switch) bandwidth: 25, radix: 64, count: 1 (added cost: $20800)\n",
      "[CostModel] Adding (inter-Node, Nic) bandwidth: 25, radix: 1, count: 64 (added cost: $0)\n",
      "Success in opening system file\n",
      "Var is: scheduling-policy: ,val is: FIFO\n",
      "Var is: endpoint-delay: ,val is: 10\n",
      "Var is: active-chunks-per-dimension: ,val is: 1\n",
      "Var is: preferred-dataset-splits: ,val is: 4\n",
      "Var is: all-reduce-implementation: ,val is: oneDirect_oneDirect_oneDirect\n",
      "Var is: all-gather-implementation: ,val is: oneDirect\n",
      "Var is: reduce-scatter-implementation: ,val is: oneRing_oneDirect\n",
      "Var is: all-to-all-implementation: ,val is: oneDirect\n",
      "Var is: collective-optimization: ,val is: localBWAware\n",
      "Var is: boost-mode: ,val is: 100\n",
      "Var is: intra-dimension-scheduling: ,val is: FIFO\n",
      "Var is: inter-dimension-scheduling: ,val is: ascending\n",
      "Var is: enable-roofline: ,val is: true\n",
      "Var is: roofline-bw-in-bytes-per-ns: ,val is: 3350\n",
      "Var is: roofline-neg-y-intercept: ,val is: 0.0\n",
      "Var is: roofline-peak-perf-in-gigaflops: ,val is: 990000\n",
      "Var is:  ,val is: 990000\n",
      "The final active chunks per dimension after allocating to queues is: 100000000\n",
      "ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\n",
      "ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\n",
      "ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\n",
      "ring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\n",
      "total nodes: 8\n",
      "LogGP model, the local reduction delay is: 1\n",
      "LogGP model, the local reduction delay is: 1\n",
      "LogGP model, the local reduction delay is: 1\n",
      "LogGP model, the local reduction delay is: 1\n",
      "Shared bus modeling enabled? false\n",
      "LogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0.0038\n",
      "communication delay (in the case of disabled shared bus): 10\n",
      "Shared bus modeling enabled? false\n",
      "LogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0\n",
      "communication delay (in the case of disabled shared bus): 10\n",
      "Success in opening workload file\n",
      "type: MICRO ,num passes: 5 ,lines: 1 compute scale: 1 ,comm scale: 50\n",
      "stat path: /app/result_test/wow/ ,total rows: 1 ,stat row: 0\n",
      "CSV path and filename: /app/result_test/wow/detailed.csv\n",
      "Success in opening CSV file for writing the report.\n",
      "CSV path and filename: /app/result_test/wow/EndToEnd.csv\n",
      "Success in opening CSV file for writing the report.\n",
      "CSV path and filename: /app/result_test/wow/backend_end_to_end.csv\n",
      "Success in opening CSV file for writing the report.\n",
      "CSV path and filename: /app/result_test/wow/backend_dim_info.csv\n",
      "Success in opening CSV file for writing the report.\n",
      "info: no weight grad collective for layer: allreduce\n",
      "info: no weight grad collective for layer: allreduce\n",
      "info: no weight grad collective for layer: allreduce\n",
      "info: no weight grad collective for layer: allreduce\n",
      "info: no weight grad collective for layer: allreduce\n",
      "*******************\n",
      "Layer id: allreduce\n",
      "Total collectives issued for this layer: 0\n",
      "*************************  Workload stats  ************************* allreduce\n",
      "id: allreduce ,Total cycles spent on fwd pass compute: 0\n",
      "id: allreduce ,Total cycles spent on weight grad compute: 0\n",
      "id: allreduce ,Total cycles spent on input grad compute: 0\n",
      "id: allreduce ,Total cycles spent idle waiting for fwd finish: 0\n",
      "id: allreduce ,Total cycles spent idle waiting for weight grad finish: 0\n",
      "id: allreduce ,Total cycles spent idle waiting for input grad finish: 0\n",
      "id: allreduce ,Total cycles spent on fwd pass comm: 0\n",
      "id: allreduce ,Total cycles spent on weight grad comm: 0\n",
      "id: allreduce ,Total cycles spent on input grad comm: 0\n",
      "*************************  Queuing stats  ************************* allreduce\n",
      "*************************  Network stats  ************************* allreduce\n",
      "*************************  Chunk Stats Per Logical Dimension (for all layers) ************************* allreduce\n",
      " ,Average chunk latency for logical dimension  1 of topology: -nan\n",
      "*************************\n",
      "all passes finished at time: 0, id of first layer: allreduce\n",
      "path to create csvs is: /app/result_test/wow/\n",
      "success in openning file\n",
      "\n",
      "[Analytical, main] Total Cost: $27200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( \"[CostModel] Adding (inter-Node, Link) bandwidth: 25, radix: 1, count: 64 (added cost: $6400)\\n[CostModel] Adding (inter-Node, Switch) bandwidth: 25, radix: 64, count: 1 (added cost: $20800)\\n[CostModel] Adding (inter-Node, Nic) bandwidth: 25, radix: 1, count: 64 (added cost: $0)\\nSuccess in opening system file\\nVar is: scheduling-policy: ,val is: FIFO\\nVar is: endpoint-delay: ,val is: 10\\nVar is: active-chunks-per-dimension: ,val is: 1\\nVar is: preferred-dataset-splits: ,val is: 4\\nVar is: all-reduce-implementation: ,val is: oneDirect_oneDirect_oneDirect\\nVar is: all-gather-implementation: ,val is: oneDirect\\nVar is: reduce-scatter-implementation: ,val is: oneRing_oneDirect\\nVar is: all-to-all-implementation: ,val is: oneDirect\\nVar is: collective-optimization: ,val is: localBWAware\\nVar is: boost-mode: ,val is: 100\\nVar is: intra-dimension-scheduling: ,val is: FIFO\\nVar is: inter-dimension-scheduling: ,val is: ascending\\nVar is: enable-roofline: ,val is: true\\nVar is: roofline-bw-in-bytes-per-ns: ,val is: 3350\\nVar is: roofline-neg-y-intercept: ,val is: 0.0\\nVar is: roofline-peak-perf-in-gigaflops: ,val is: 990000\\nVar is:  ,val is: 990000\\nThe final active chunks per dimension after allocating to queues is: 100000000\\nring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\\nring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\\nring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\\nring of node 0, id: 0 dimension: local total nodes in ring: 8 index in ring: 0 offset: 1total nodes in ring: 8\\ntotal nodes: 8\\nLogGP model, the local reduction delay is: 1\\nLogGP model, the local reduction delay is: 1\\nLogGP model, the local reduction delay is: 1\\nLogGP model, the local reduction delay is: 1\\nShared bus modeling enabled? false\\nLogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0.0038\\ncommunication delay (in the case of disabled shared bus): 10\\nShared bus modeling enabled? false\\nLogGP model, the L is:0 ,o is: 0 ,g is: 0 ,G is: 0\\ncommunication delay (in the case of disabled shared bus): 10\\nSuccess in opening workload file\\ntype: MICRO ,num passes: 5 ,lines: 1 compute scale: 1 ,comm scale: 50\\nstat path: /app/result_test/wow/ ,total rows: 1 ,stat row: 0\\nCSV path and filename: /app/result_test/wow/detailed.csv\\nSuccess in opening CSV file for writing the report.\\nCSV path and filename: /app/result_test/wow/EndToEnd.csv\\nSuccess in opening CSV file for writing the report.\\nCSV path and filename: /app/result_test/wow/backend_end_to_end.csv\\nSuccess in opening CSV file for writing the report.\\nCSV path and filename: /app/result_test/wow/backend_dim_info.csv\\nSuccess in opening CSV file for writing the report.\\ninfo: no weight grad collective for layer: allreduce\\ninfo: no weight grad collective for layer: allreduce\\ninfo: no weight grad collective for layer: allreduce\\ninfo: no weight grad collective for layer: allreduce\\ninfo: no weight grad collective for layer: allreduce\\n*******************\\nLayer id: allreduce\\nTotal collectives issued for this layer: 0\\n*************************  Workload stats  ************************* allreduce\\nid: allreduce ,Total cycles spent on fwd pass compute: 0\\nid: allreduce ,Total cycles spent on weight grad compute: 0\\nid: allreduce ,Total cycles spent on input grad compute: 0\\nid: allreduce ,Total cycles spent idle waiting for fwd finish: 0\\nid: allreduce ,Total cycles spent idle waiting for weight grad finish: 0\\nid: allreduce ,Total cycles spent idle waiting for input grad finish: 0\\nid: allreduce ,Total cycles spent on fwd pass comm: 0\\nid: allreduce ,Total cycles spent on weight grad comm: 0\\nid: allreduce ,Total cycles spent on input grad comm: 0\\n*************************  Queuing stats  ************************* allreduce\\n*************************  Network stats  ************************* allreduce\\n*************************  Chunk Stats Per Logical Dimension (for all layers) ************************* allreduce\\n ,Average chunk latency for logical dimension  1 of topology: -nan\\n*************************\\nall passes finished at time: 0, id of first layer: allreduce\\npath to create csvs is: /app/result_test/wow/\\nsuccess in openning file\\n\\n[Analytical, main] Total Cost: $27200\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
